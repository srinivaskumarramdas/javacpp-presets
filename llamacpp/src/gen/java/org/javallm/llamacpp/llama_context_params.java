// Targeted by JavaCPP version 1.5.10-SNAPSHOT: DO NOT EDIT THIS FILE

package org.javallm.llamacpp;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

import static org.javallm.llamacpp.global.llama.*;


   @Properties(inherit = org.javallm.llamacpp.presets.llama.class)
public class llama_context_params extends Pointer {
       static { Loader.load(); }
       /** Default native constructor. */
       public llama_context_params() { super((Pointer)null); allocate(); }
       /** Native array allocator. Access with {@link Pointer#position(long)}. */
       public llama_context_params(long size) { super((Pointer)null); allocateArray(size); }
       /** Pointer cast constructor. Invokes {@link Pointer#Pointer(Pointer)}. */
       public llama_context_params(Pointer p) { super(p); }
       private native void allocate();
       private native void allocateArray(long size);
       @Override public llama_context_params position(long position) {
           return (llama_context_params)super.position(position);
       }
       @Override public llama_context_params getPointer(long i) {
           return new llama_context_params((Pointer)this).offsetAddress(i);
       }
   
        public native @Cast("uint32_t") int seed(); public native llama_context_params seed(int setter);                         // RNG seed, -1 for random
        public native int n_ctx(); public native llama_context_params n_ctx(int setter);                        // text context
        public native int n_batch(); public native llama_context_params n_batch(int setter);                      // prompt processing batch size
        public native int n_gpu_layers(); public native llama_context_params n_gpu_layers(int setter);                 // number of layers to store in VRAM
        public native int main_gpu(); public native llama_context_params main_gpu(int setter);                     // the GPU that is used for scratch and small tensors
        public native float tensor_split(int i); public native llama_context_params tensor_split(int i, float setter);
        @MemberGetter public native FloatPointer tensor_split(); // how to split layers across multiple GPUs
        // called with a progress value between 0 and 1, pass NULL to disable
        public native llama_progress_callback progress_callback(); public native llama_context_params progress_callback(llama_progress_callback setter);
        // context pointer passed to the progress callback
        public native Pointer progress_callback_user_data(); public native llama_context_params progress_callback_user_data(Pointer setter);

        // Keep the booleans together to avoid misalignment during copy-by-value.
        public native @Cast("bool") boolean low_vram(); public native llama_context_params low_vram(boolean setter);   // if true, reduce VRAM usage at the cost of performance
        public native @Cast("bool") boolean f16_kv(); public native llama_context_params f16_kv(boolean setter);     // use fp16 for KV cache
        public native @Cast("bool") boolean logits_all(); public native llama_context_params logits_all(boolean setter); // the llama_eval() call computes all logits, not just the last one
        public native @Cast("bool") boolean vocab_only(); public native llama_context_params vocab_only(boolean setter); // only load the vocabulary, no weights
        public native @Cast("bool") boolean use_mmap(); public native llama_context_params use_mmap(boolean setter);   // use mmap if possible
        public native @Cast("bool") boolean use_mlock(); public native llama_context_params use_mlock(boolean setter);  // force system to keep model in RAM
        public native @Cast("bool") boolean embedding(); public native llama_context_params embedding(boolean setter);  // embedding mode only
    }
