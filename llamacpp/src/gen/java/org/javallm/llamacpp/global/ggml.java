// Targeted by JavaCPP version 1.5.10-SNAPSHOT: DO NOT EDIT THIS FILE

package org.javallm.llamacpp.global;

import org.javallm.llamacpp.*;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

public class ggml extends org.javallm.llamacpp.presets.ggml {
    static { Loader.load(); }

// Parsed from ggml.h

// #pragma once

//
// GGML Tensor Library
//
// This documentation is still a work in progress.
// If you wish some specific topics to be covered, feel free to drop a comment:
//
//   https://github.com/ggerganov/whisper.cpp/issues/40
//
// ## Overview
//
// This library implements:
//
//  - a set of tensor operations
//  - automatic differentiation
//  - basic optimization algorithms
//
// The aim of this library is to provide a minimalistic approach for various machine learning tasks. This includes,
// but is not limited to, the following:
//
//  - linear regression
//  - support vector machines
//  - neural networks
//
// The library allows the user to define a certain function using the available tensor operations. This function
// definition is represented internally via a computation graph. Each tensor operation in the function definition
// corresponds to a node in the graph. Having the computation graph defined, the user can choose to compute the
// function's value and/or its gradient with respect to the input variables. Optionally, the function can be optimized
// using one of the available optimization algorithms.
//
// For example, here we define the function: f(x) = a*x^2 + b
//
//   {
//       struct ggml_init_params params = {
//           .mem_size   = 16*1024*1024,
//           .mem_buffer = NULL,
//       };
//
//       // memory allocation happens here
//       struct ggml_context * ctx = ggml_init(params);
//
//       struct ggml_tensor * x = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//
//       ggml_set_param(ctx, x); // x is an input variable
//
//       struct ggml_tensor * a  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//       struct ggml_tensor * b  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//       struct ggml_tensor * x2 = ggml_mul(ctx, x, x);
//       struct ggml_tensor * f  = ggml_add(ctx, ggml_mul(ctx, a, x2), b);
//
//       ...
//   }
//
// Notice that the function definition above does not involve any actual computation. The computation is performed only
// when the user explicitly requests it. For example, to compute the function's value at x = 2.0:
//
//   {
//       ...
//
//       struct ggml_cgraph gf = ggml_build_forward(f);
//
//       // set the input variable and parameter values
//       ggml_set_f32(x, 2.0f);
//       ggml_set_f32(a, 3.0f);
//       ggml_set_f32(b, 4.0f);
//
//       ggml_graph_compute_with_ctx(ctx, &gf, n_threads);
//
//       printf("f = %f\n", ggml_get_f32_1d(f, 0));
//
//       ...
//   }
//
// The actual computation is performed in the ggml_graph_compute() function.
//
// The ggml_new_tensor_...() functions create new tensors. They are allocated in the memory buffer provided to the
// ggml_init() function. You have to be careful not to exceed the memory buffer size. Therefore, you have to know
// in advance how much memory you need for your computation. Alternatively, you can allocate a large enough memory
// and after defining the computation graph, call the ggml_used_mem() function to find out how much memory was
// actually needed.
//
// The ggml_set_param() function marks a tensor as an input variable. This is used by the automatic
// differentiation and optimization algorithms.
//
// The described approach allows to define the function graph once and then compute its forward or backward graphs
// multiple times. All computations will use the same memory buffer allocated in the ggml_init() function. This way
// the user can avoid the memory allocation overhead at runtime.
//
// The library supports multi-dimensional tensors - up to 4 dimensions. The FP16 and FP32 data types are first class
// citizens, but in theory the library can be extended to support FP8 and integer data types.
//
// Each tensor operation produces a new tensor. Initially the library was envisioned to support only the use of unary
// and binary operations. Most of the available operations fall into one of these two categories. With time, it became
// clear that the library needs to support more complex operations. The way to support these operations is not clear
// yet, but a few examples are demonstrated in the following operations:
//
//   - ggml_permute()
//   - ggml_conv_1d_1s()
//   - ggml_conv_1d_2s()
//
// For each tensor operator, the library implements a forward and backward computation function. The forward function
// computes the output tensor value given the input tensor values. The backward function computes the adjoint of the
// input tensors given the adjoint of the output tensor. For a detailed explanation of what this means, take a
// calculus class, or watch the following video:
//
//   What is Automatic Differentiation?
//   https://www.youtube.com/watch?v=wG_nF1awSSY
//
//
// ## Tensor data (struct ggml_tensor)
//
// The tensors are stored in memory via the ggml_tensor struct. The structure provides information about the size of
// the tensor, the data type, and the memory buffer where the tensor data is stored. Additionally, it contains
// pointers to the "source" tensors - i.e. the tensors that were used to compute the current tensor. For example:
//
//   {
//       struct ggml_tensor * c = ggml_add(ctx, a, b);
//
//       assert(c->src[0] == a);
//       assert(c->src[1] == b);
//   }
//
// The multi-dimensional tensors are stored in row-major order. The ggml_tensor struct contains fields for the
// number of elements in each dimension ("ne") as well as the number of bytes ("nb", a.k.a. stride). This allows
// to store tensors that are not contiguous in memory, which is useful for operations such as transposition and
// permutation. All tensor operations have to take the stride into account and not assume that the tensor is
// contiguous in memory.
//
// The data of the tensor is accessed via the "data" pointer. For example:
//
//   {
//       struct ggml_tensor * a = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, 2, 3);
//
//       // a[2, 1] = 1.0f;
//       *(float *) ((char *) a->data + 2*a->nb[1] + 1*a->nb[0]) = 1.0f;
//
//       // a[0, 2] = 2.0f;
//       *(float *) ((char *) a->data + 0*a->nb[1] + 2*a->nb[0]) = 2.0f;
//
//       ...
//   }
//
// Alternatively, there are helper functions, such as ggml_get_f32_1d() and ggml_set_f32_1d() that can be used.
//
// ## The matrix multiplication operator (ggml_mul_mat)
//
// TODO
//
//
// ## Multi-threading
//
// TODO
//
//
// ## Overview of ggml.c
//
// TODO
//
//
// ## SIMD optimizations
//
// TODO
//
//
// ## Debugging ggml
//
// TODO
//
//

// #ifdef GGML_SHARED
// #else
// #    define GGML_API
// #endif

// TODO: support for clang
// #ifdef __GNUC__
// #    define GGML_DEPRECATED(func, hint) func __attribute__((deprecated(hint)))
// #elif defined(_MSC_VER)
// #    define GGML_DEPRECATED(func, hint) __declspec(deprecated(hint)) func
// #else
// #    define GGML_DEPRECATED(func, hint) func
// #endif

// #include <stdint.h>
// #include <stddef.h>
// #include <stdbool.h>

public static final int GGML_FILE_MAGIC =   0x67676d6c; // "ggml"
public static final int GGML_FILE_VERSION = 1;

public static final int GGML_QNT_VERSION =        2;    // bump this on quantization format changes
public static final int GGML_QNT_VERSION_FACTOR = 1000; // do not change this

public static final int GGML_MAX_DIMS =          4;
public static final int GGML_MAX_NODES =         4096;
public static final int GGML_MAX_PARAMS =        256;
public static final int GGML_MAX_CONTEXTS =      64;
public static final int GGML_MAX_SRC =           6;
public static final int GGML_MAX_NAME =          64;
public static final int GGML_MAX_OP_PARAMS =     32;
public static final int GGML_DEFAULT_N_THREADS = 4;


public static final int GGML_EXIT_SUCCESS = 0;
public static final int GGML_EXIT_ABORTED = 1;

public static final int GGUF_MAGIC =   0x46554747; // "GGUF"
public static final int GGUF_VERSION = 1;

public static final int GGUF_DEFAULT_ALIGNMENT = 32;

// #define GGML_UNUSED(x) (void)(x)

// #define GGML_PAD(x, n) (((x) + (n) - 1) & ~((n) - 1))

// #define GGML_ASSERT(x)
//     do {
//         if (!(x)) {
//             fprintf(stderr, "GGML_ASSERT: %s:%d: %s\n", __FILE__, __LINE__, #x);
//             abort();
//         }
//     } while (0)

// used to copy the number of elements and stride in bytes of tensors into local variables.
// main purpose is to reduce code duplication and improve readability.
//
// example:
//
//    GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne);
//    GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb);
//
// #define GGML_TENSOR_LOCALS_1(type, prefix, pointer, array)
//     const type prefix##0 = (pointer)->array[0];
//     GGML_UNUSED(prefix##0);
// #define GGML_TENSOR_LOCALS_2(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_1    (type, prefix, pointer, array)
//     const type prefix##1 = (pointer)->array[1];
//     GGML_UNUSED(prefix##1);
// #define GGML_TENSOR_LOCALS_3(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_2    (type, prefix, pointer, array)
//     const type prefix##2 = (pointer)->array[2];
//     GGML_UNUSED(prefix##2);
// #define GGML_TENSOR_LOCALS(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_3  (type, prefix, pointer, array)
//     const type prefix##3 = (pointer)->array[3];
//     GGML_UNUSED(prefix##3);

// #ifdef  __cplusplus
// #endif

// #if defined(__ARM_NEON) && defined(__CUDACC__)
// #elif defined(__ARM_NEON)
// #else
// #endif

    // convert FP16 <-> FP32
    public static native float ggml_fp16_to_fp32(@Cast("ggml_fp16_t") short x);
    public static native @Cast("ggml_fp16_t") short ggml_fp32_to_fp16(float x);

    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") ShortPointer x, FloatPointer y, int n);
    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") ShortBuffer x, FloatBuffer y, int n);
    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") short[] x, float[] y, int n);
    public static native void ggml_fp32_to_fp16_row(@Const FloatPointer x, @Cast("ggml_fp16_t*") ShortPointer y, int n);
    public static native void ggml_fp32_to_fp16_row(@Const FloatBuffer x, @Cast("ggml_fp16_t*") ShortBuffer y, int n);
    public static native void ggml_fp32_to_fp16_row(@Const float[] x, @Cast("ggml_fp16_t*") short[] y, int n);
// Targeting ../ggml_context.java



    /** enum ggml_type */
    public static final int
        GGML_TYPE_F32  = 0,
        GGML_TYPE_F16  = 1,
        GGML_TYPE_Q4_0 = 2,
        GGML_TYPE_Q4_1 = 3,
        // GGML_TYPE_Q4_2 = 4, support has been removed
        // GGML_TYPE_Q4_3 (5) support has been removed
        GGML_TYPE_Q5_0 = 6,
        GGML_TYPE_Q5_1 = 7,
        GGML_TYPE_Q8_0 = 8,
        GGML_TYPE_Q8_1 = 9,
        // k-quantizations
        GGML_TYPE_Q2_K = 10,
        GGML_TYPE_Q3_K = 11,
        GGML_TYPE_Q4_K = 12,
        GGML_TYPE_Q5_K = 13,
        GGML_TYPE_Q6_K = 14,
        GGML_TYPE_Q8_K = 15,
        GGML_TYPE_I8 = 16,
        GGML_TYPE_I16 = 17,
        GGML_TYPE_I32 = 18,
        GGML_TYPE_COUNT = 19;

    /** enum ggml_backend */
    public static final int
        GGML_BACKEND_CPU = 0,
        GGML_BACKEND_GPU = 10,
        GGML_BACKEND_GPU_SPLIT = 20;

    // model file types
    /** enum ggml_ftype */
    public static final int
        GGML_FTYPE_UNKNOWN     = -1,
        GGML_FTYPE_ALL_F32     = 0,
        GGML_FTYPE_MOSTLY_F16  = 1,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_0 = 2,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_1 = 3,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4, // tok_embeddings.weight and output.weight are F16
        GGML_FTYPE_MOSTLY_Q8_0 = 7,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_0 = 8,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_1 = 9,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q2_K = 10, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q3_K = 11, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_K = 12, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_K = 13, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q6_K = 14; // except 1d tensors

    // available tensor operations:
    /** enum ggml_op */
    public static final int
        GGML_OP_NONE = 0,

        GGML_OP_DUP = 1,
        GGML_OP_ADD = 2,
        GGML_OP_ADD1 = 3,
        GGML_OP_ACC = 4,
        GGML_OP_SUB = 5,
        GGML_OP_MUL = 6,
        GGML_OP_DIV = 7,
        GGML_OP_SQR = 8,
        GGML_OP_SQRT = 9,
        GGML_OP_LOG = 10,
        GGML_OP_SUM = 11,
        GGML_OP_SUM_ROWS = 12,
        GGML_OP_MEAN = 13,
        GGML_OP_ARGMAX = 14,
        GGML_OP_REPEAT = 15,
        GGML_OP_REPEAT_BACK = 16,
        GGML_OP_CONCAT = 17,
        GGML_OP_SILU_BACK = 18,
        GGML_OP_NORM = 19, // normalize
        GGML_OP_RMS_NORM = 20,
        GGML_OP_RMS_NORM_BACK = 21,
        GGML_OP_GROUP_NORM = 22,

        GGML_OP_MUL_MAT = 23,
        GGML_OP_OUT_PROD = 24,

        GGML_OP_SCALE = 25,
        GGML_OP_SET = 26,
        GGML_OP_CPY = 27,
        GGML_OP_CONT = 28,
        GGML_OP_RESHAPE = 29,
        GGML_OP_VIEW = 30,
        GGML_OP_PERMUTE = 31,
        GGML_OP_TRANSPOSE = 32,
        GGML_OP_GET_ROWS = 33,
        GGML_OP_GET_ROWS_BACK = 34,
        GGML_OP_DIAG = 35,
        GGML_OP_DIAG_MASK_INF = 36,
        GGML_OP_DIAG_MASK_ZERO = 37,
        GGML_OP_SOFT_MAX = 38,
        GGML_OP_SOFT_MAX_BACK = 39,
        GGML_OP_ROPE = 40,
        GGML_OP_ROPE_BACK = 41,
        GGML_OP_ALIBI = 42,
        GGML_OP_CLAMP = 43,
        GGML_OP_CONV_1D = 44,
        GGML_OP_CONV_2D = 45,
        GGML_OP_CONV_TRANSPOSE_2D = 46,
        GGML_OP_POOL_1D = 47,
        GGML_OP_POOL_2D = 48,

        GGML_OP_UPSCALE = 49, // nearest interpolate

        GGML_OP_FLASH_ATTN = 50,
        GGML_OP_FLASH_FF = 51,
        GGML_OP_FLASH_ATTN_BACK = 52,
        GGML_OP_WIN_PART = 53,
        GGML_OP_WIN_UNPART = 54,
        GGML_OP_GET_REL_POS = 55,
        GGML_OP_ADD_REL_POS = 56,

        GGML_OP_UNARY = 57,

        GGML_OP_MAP_UNARY = 58,
        GGML_OP_MAP_BINARY = 59,

        GGML_OP_MAP_CUSTOM1_F32 = 60,
        GGML_OP_MAP_CUSTOM2_F32 = 61,
        GGML_OP_MAP_CUSTOM3_F32 = 62,

        GGML_OP_MAP_CUSTOM1 = 63,
        GGML_OP_MAP_CUSTOM2 = 64,
        GGML_OP_MAP_CUSTOM3 = 65,

        GGML_OP_CROSS_ENTROPY_LOSS = 66,
        GGML_OP_CROSS_ENTROPY_LOSS_BACK = 67,

        GGML_OP_COUNT = 68;

    /** enum ggml_unary_op */
    public static final int
        GGML_UNARY_OP_ABS = 0,
        GGML_UNARY_OP_SGN = 1,
        GGML_UNARY_OP_NEG = 2,
        GGML_UNARY_OP_STEP = 3,
        GGML_UNARY_OP_TANH = 4,
        GGML_UNARY_OP_ELU = 5,
        GGML_UNARY_OP_RELU = 6,
        GGML_UNARY_OP_GELU = 7,
        GGML_UNARY_OP_GELU_QUICK = 8,
        GGML_UNARY_OP_SILU = 9;

    /** enum ggml_object_type */
    public static final int
        GGML_OBJECT_TENSOR = 0,
        GGML_OBJECT_GRAPH = 1,
        GGML_OBJECT_WORK_BUFFER = 2;
// Targeting ../ggml_object.java



    @MemberGetter public static native @Cast("const size_t") long GGML_OBJECT_SIZE();
    public static final long GGML_OBJECT_SIZE = GGML_OBJECT_SIZE();
// Targeting ../ggml_tensor.java



    @MemberGetter public static native @Cast("const size_t") long GGML_TENSOR_SIZE();
    public static final long GGML_TENSOR_SIZE = GGML_TENSOR_SIZE();
// Targeting ../ggml_cplan.java



    // next prime after GGML_MAX_NODES
    // #define GGML_GRAPH_HASHTABLE_SIZE 4099
    // next prime after GGML_MAX_NODES * 2 (nodes + leafs)
    public static final int GGML_GRAPH_HASHTABLE_SIZE = 8273;
// Targeting ../ggml_cgraph.java



    @MemberGetter public static native @Cast("const size_t") long GGML_GRAPH_SIZE();
    public static final long GGML_GRAPH_SIZE = GGML_GRAPH_SIZE();
// Targeting ../ggml_scratch.java


// Targeting ../ggml_init_params.java




    // compute types

    // NOTE: the INIT or FINALIZE pass is not scheduled unless explicitly enabled.
    // This behavior was changed since https://github.com/ggerganov/llama.cpp/pull/1995.
    /** enum ggml_task_type */
    public static final int
        GGML_TASK_INIT = 0,
        GGML_TASK_COMPUTE = 1,
        GGML_TASK_FINALIZE = 2;
// Targeting ../ggml_compute_params.java



    // misc

    public static native void ggml_time_init(); // call this once at the beginning of the program
    public static native @Cast("int64_t") long ggml_time_ms();
    public static native @Cast("int64_t") long ggml_time_us();
    public static native @Cast("int64_t") long ggml_cycles();
    public static native @Cast("int64_t") long ggml_cycles_per_ms();

    public static native void ggml_numa_init(); // call once for better performance on NUMA systems
    public static native @Cast("bool") boolean ggml_is_numa(); // true if init detected that system has >1 NUMA node

    public static native void ggml_print_object(@Const ggml_object obj);
    public static native void ggml_print_objects(@Const ggml_context ctx);

    public static native @Cast("int64_t") long ggml_nelements(@Const ggml_tensor tensor);
    public static native @Cast("int64_t") long ggml_nrows(@Const ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_nbytes(@Const ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_nbytes_pad(@Const ggml_tensor tensor); // same as ggml_nbytes() but padded to GGML_MEM_ALIGN
    public static native @Cast("size_t") long ggml_nbytes_split(@Const ggml_tensor tensor, int nrows_split);

    public static native int ggml_blck_size(@Cast("ggml_type") int type);
    public static native @Cast("size_t") long ggml_type_size(@Cast("ggml_type") int type); // size in bytes for all elements in a block
    public static native float ggml_type_sizef(@Cast("ggml_type") int type); // ggml_type_size()/ggml_blck_size() as float

    public static native @Cast("const char*") BytePointer ggml_type_name(@Cast("ggml_type") int type);
    public static native @Cast("const char*") BytePointer ggml_op_name(@Cast("ggml_op") int op);
    public static native @Cast("const char*") BytePointer ggml_op_symbol(@Cast("ggml_op") int op);

    public static native @Cast("size_t") long ggml_element_size(@Const ggml_tensor tensor);

    public static native @Cast("bool") boolean ggml_is_quantized(@Cast("ggml_type") int type);

    // TODO: temporary until model loading of ggml examples is refactored
    public static native @Cast("ggml_type") int ggml_ftype_to_ggml_type(@Cast("ggml_ftype") int ftype);

    public static native @Cast("bool") boolean ggml_is_transposed(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_contiguous(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_permuted(@Const ggml_tensor tensor);

    public static native @Cast("bool") boolean ggml_are_same_shape(@Const ggml_tensor t0, @Const ggml_tensor t1);

    // use this to compute the memory overhead of a tensor
    public static native @Cast("size_t") long ggml_tensor_overhead();

    // main

    public static native ggml_context ggml_init(@ByVal ggml_init_params params);
    public static native void ggml_free(ggml_context ctx);

    public static native @Cast("size_t") long ggml_used_mem(@Const ggml_context ctx);

    public static native @Cast("size_t") long ggml_set_scratch(ggml_context ctx, @ByVal ggml_scratch scratch);
    public static native @Cast("bool") boolean ggml_get_no_alloc(ggml_context ctx);
    public static native void ggml_set_no_alloc(ggml_context ctx, @Cast("bool") boolean no_alloc);

    public static native Pointer ggml_get_mem_buffer(@Const ggml_context ctx);
    public static native @Cast("size_t") long ggml_get_mem_size(@Const ggml_context ctx);
    public static native @Cast("size_t") long ggml_get_max_tensor_size(@Const ggml_context ctx);

    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") LongPointer ne);
    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") LongBuffer ne);
    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") long[] ne);

    public static native ggml_tensor ggml_new_tensor_1d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0);

    public static native ggml_tensor ggml_new_tensor_2d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1);

    public static native ggml_tensor ggml_new_tensor_3d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2);

    public static native ggml_tensor ggml_new_tensor_4d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3);

    public static native ggml_tensor ggml_new_i32(ggml_context ctx, int value);
    public static native ggml_tensor ggml_new_f32(ggml_context ctx, float value);

    public static native ggml_tensor ggml_dup_tensor(ggml_context ctx, @Const ggml_tensor src);
    public static native ggml_tensor ggml_view_tensor(ggml_context ctx, @Const ggml_tensor src);

    public static native ggml_tensor ggml_get_tensor(ggml_context ctx, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_get_tensor(ggml_context ctx, String name);

    public static native ggml_tensor ggml_set_zero(ggml_tensor tensor);
    public static native ggml_tensor ggml_set_i32(ggml_tensor tensor, int value);
    public static native ggml_tensor ggml_set_f32(ggml_tensor tensor, float value);

    public static native int ggml_get_i32_1d(@Const ggml_tensor tensor, int i);
    public static native void ggml_set_i32_1d(@Const ggml_tensor tensor, int i, int value);

    public static native float ggml_get_f32_1d(@Const ggml_tensor tensor, int i);
    public static native void ggml_set_f32_1d(@Const ggml_tensor tensor, int i, float value);

    public static native Pointer ggml_get_data(@Const ggml_tensor tensor);
    public static native FloatPointer ggml_get_data_f32(@Const ggml_tensor tensor);

    public static native @Cast("ggml_unary_op") int ggml_get_unary_op(@Const ggml_tensor tensor);

    public static native @Cast("const char*") BytePointer ggml_get_name(@Const ggml_tensor tensor);
    public static native ggml_tensor ggml_set_name(      ggml_tensor tensor, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_set_name(      ggml_tensor tensor, String name);
    public static native ggml_tensor ggml_format_name(      ggml_tensor tensor, @Cast("const char*") BytePointer fmt);
    public static native ggml_tensor ggml_format_name(      ggml_tensor tensor, String fmt);

    //
    // operations on tensors with backpropagation
    //

    public static native ggml_tensor ggml_dup(
                ggml_context ctx,
                ggml_tensor a);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_dup_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_add(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add1(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add1_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_acc(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_acc_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_sub(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_sub_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_mul(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_mul_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_div(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_div_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_sqr(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqr_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqrt(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqrt_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_log(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_log_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // return scalar
    public static native ggml_tensor ggml_sum(
                ggml_context ctx,
                ggml_tensor a);

    // sums along rows, with input shape [a,b,c,d] return shape [1,b,c,d]
    public static native ggml_tensor ggml_sum_rows(
                ggml_context ctx,
                ggml_tensor a);

    // mean along rows
    public static native ggml_tensor ggml_mean(
                ggml_context ctx,
                ggml_tensor a);

    // argmax along rows
    public static native ggml_tensor ggml_argmax(
                ggml_context ctx,
                ggml_tensor a);

    // if a is the same shape as b, and a is not parameter, return a
    // otherwise, return a new tensor: repeat(a) to fit in b
    public static native ggml_tensor ggml_repeat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_repeat_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // concat a and b on dim 2
    // used in stable-diffusion
    public static native ggml_tensor ggml_concat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_abs(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_abs_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sgn(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sgn_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_neg(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_neg_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_step(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_step_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_tanh(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_tanh_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_elu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_elu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_relu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_relu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // TODO: double-check this computation is correct
    public static native ggml_tensor ggml_gelu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_quick(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_quick_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_silu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_silu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // a - x
    // b - dy
    public static native ggml_tensor ggml_silu_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // normalize along rows
    // TODO: eps is hardcoded to 1e-5 for now
    public static native ggml_tensor ggml_norm(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_norm_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_rms_norm(
                ggml_context ctx,
                ggml_tensor a,
                float eps);

    public static native ggml_tensor ggml_rms_norm_inplace(
                ggml_context ctx,
                ggml_tensor a,
                float eps);

    // group normalize along ne0*ne1*n_groups
    // used in stable-diffusion
    // TODO: eps is hardcoded to 1e-6 for now
    public static native ggml_tensor ggml_group_norm(
                ggml_context ctx,
                ggml_tensor a,
                int n_groups);

    public static native ggml_tensor ggml_group_norm_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_groups);

    // a - x
    // b - dy
    // TODO: update with configurable eps
    public static native ggml_tensor ggml_rms_norm_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // A: n columns, m rows
    // B: n columns, p rows  (i.e. we transpose it internally)
    // result is m columns, p rows
    public static native ggml_tensor ggml_mul_mat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // A: m columns, n rows,
    // B: p columns, n rows,
    // result is m columns, p rows
    public static native ggml_tensor ggml_out_prod(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    //
    // operations on tensors without backpropagation
    //

    public static native ggml_tensor ggml_scale(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_scale_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // b -> view(a,offset,nb1,nb2,3), return modified a
    public static native ggml_tensor ggml_set(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    // b -> view(a,offset,nb1,nb2,3), return view(a)
    public static native ggml_tensor ggml_set_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_set_1d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_set_1d_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long offset);

    // b -> view(a,offset,nb1,nb2,3), return modified a
    public static native ggml_tensor ggml_set_2d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset);

    // b -> view(a,offset,nb1,nb2,3), return view(a)
    public static native ggml_tensor ggml_set_2d_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset);


    // a -> b, return view(b)
    public static native ggml_tensor ggml_cpy(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // a -> b, in-place, return view(b)
    public static native ggml_tensor ggml_cpy_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // make contiguous
    public static native ggml_tensor ggml_cont(
                ggml_context ctx,
                ggml_tensor a);

    // make contiguous, in-place
    public static native ggml_tensor ggml_cont_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // return view(a), b specifies the new shape
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // return view(a)
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0);

    public static native ggml_tensor ggml_reshape_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1);

    // return view(a)
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape_3d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2);

    public static native ggml_tensor ggml_reshape_4d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3);

    // offset in bytes
    public static native ggml_tensor ggml_view_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_3d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_4d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_permute(
                ggml_context ctx,
                ggml_tensor a,
                int axis0,
                int axis1,
                int axis2,
                int axis3);

    // alias for ggml_permute(ctx, a, 1, 0, 2, 3)
    public static native ggml_tensor ggml_transpose(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_get_rows(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_get_rows_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c);

    public static native ggml_tensor ggml_diag(
            ggml_context ctx,
            ggml_tensor a);

    // set elements above the diagonal to -INF
    public static native ggml_tensor ggml_diag_mask_inf(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_diag_mask_inf_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // set elements above the diagonal to 0
    public static native ggml_tensor ggml_diag_mask_zero(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_diag_mask_zero_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    public static native ggml_tensor ggml_soft_max(
                ggml_context ctx,
                ggml_tensor a);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_soft_max_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_soft_max_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_soft_max_back_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // rotary position embedding
    // if mode & 1 == 1, skip n_past elements
    // if mode & 2 == 1, GPT-NeoX style
    // if mode & 4 == 1, ChatGLM style
    // TODO: avoid creating a new tensor every time
    public static native ggml_tensor ggml_rope(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                int mode,
                int n_ctx);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_rope_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                int mode,
                int n_ctx);

    // custom RoPE
    public static native ggml_tensor ggml_rope_custom(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                int mode,
                int n_ctx,
                float freq_base,
                float freq_scale);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_rope_custom_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                int mode,
                int n_ctx,
                float freq_base,
                float freq_scale);

    // xPos RoPE, in-place, returns view(a)
    public static native ggml_tensor ggml_rope_xpos_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                float base,
                @Cast("bool") boolean down);

    // rotary position embedding backward, i.e compute dx from dy
    // a - dy
    public static native ggml_tensor ggml_rope_back(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                int mode,
                int n_ctx,
                float freq_base,
                float freq_scale,
                float xpos_base,
                @Cast("bool") boolean xpos_down);

    // alibi position embedding
    // in-place, returns view(a)
    public static native ggml_tensor ggml_alibi(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_head,
                float bias_max);

    // clamp
    // in-place, returns view(a)
    public static native ggml_tensor ggml_clamp(
                ggml_context ctx,
                ggml_tensor a,
                float min,
                float max);

    public static native ggml_tensor ggml_conv_1d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int p0,
                int d0); // dilation

    // conv_1d with padding = half
    // alias for ggml_conv_1d(a, b, s, a->ne[0]/2, d)
    public static native ggml_tensor ggml_conv_1d_ph(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s,
                int d);

    public static native ggml_tensor ggml_conv_2d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int s1,
                int p0,
                int p1,
                int d0,
                int d1);


    // kernel size is a->ne[0] x a->ne[1]
    // stride is equal to kernel size
    // padding is zero
    // example:
    // a:     16   16    3  768
    // b:   1024 1024    3    1
    // res:   64   64  768    1
    // used in sam
    public static native ggml_tensor ggml_conv_2d_sk_p0(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // kernel size is a->ne[0] x a->ne[1]
    // stride is 1
    // padding is half
    // example:
    // a:      3    3    256  256
    // b:     64   64    256    1
    // res:   64   64    256    1
    // used in sam
    public static native ggml_tensor ggml_conv_2d_s1_ph(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_conv_transpose_2d_p0(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int stride);

    /** enum ggml_op_pool */
    public static final int
        GGML_OP_POOL_MAX = 0,
        GGML_OP_POOL_AVG = 1,
        GGML_OP_POOL_COUNT = 2;

    public static native ggml_tensor ggml_pool_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("ggml_op_pool") int op,
                int k0,
                int s0,
                int p0); // padding

    public static native ggml_tensor ggml_pool_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("ggml_op_pool") int op,
                int k0,
                int k1,
                int s0,
                int s1,
                int p0,
                int p1);

    // nearest interpolate
    // used in stable-diffusion
    public static native ggml_tensor ggml_upscale(
                ggml_context ctx,
                ggml_tensor a,
                int scale_factor);

    public static native ggml_tensor ggml_flash_attn(
                ggml_context ctx,
                ggml_tensor q,
                ggml_tensor k,
                ggml_tensor v,
                @Cast("bool") boolean masked);

    public static native ggml_tensor ggml_flash_attn_back(
               ggml_context ctx,
               ggml_tensor q,
               ggml_tensor k,
               ggml_tensor v,
               ggml_tensor d,
               @Cast("bool") boolean masked);

    public static native ggml_tensor ggml_flash_ff(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b0,
                ggml_tensor b1,
                ggml_tensor c0,
                ggml_tensor c1);

    // partition into non-overlapping windows with padding if needed
    // example:
    // a:   768   64   64    1
    // w:    14
    // res: 768   14   14    25
    // used in sam
    public static native ggml_tensor ggml_win_part(
                ggml_context ctx,
                ggml_tensor a,
                int w);

    // reverse of ggml_win_part
    // used in sam
    public static native ggml_tensor ggml_win_unpart(
                ggml_context ctx,
                ggml_tensor a,
                int w0,
                int h0,
                int w);

    public static native ggml_tensor ggml_unary(
                ggml_context ctx,
                 ggml_tensor a,
                 @Cast("ggml_unary_op") int op);

    public static native ggml_tensor ggml_unary_inplace(
            ggml_context ctx,
            ggml_tensor a,
            @Cast("ggml_unary_op") int op);

    // used in sam
    public static native ggml_tensor ggml_get_rel_pos(
                ggml_context ctx,
                ggml_tensor a,
                int qh,
                int kh);

    // used in sam

    public static native ggml_tensor ggml_add_rel_pos(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor pw,
                ggml_tensor ph);

    public static native ggml_tensor ggml_add_rel_pos_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor pw,
                ggml_tensor ph);
// Targeting ../ggml_unary_op_f32_t.java


// Targeting ../ggml_binary_op_f32_t.java


// Targeting ../ggml_custom1_op_f32_t.java


// Targeting ../ggml_custom2_op_f32_t.java


// Targeting ../ggml_custom3_op_f32_t.java



    public static native ggml_tensor ggml_map_unary_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_unary_op_f32_t fun);

    public static native ggml_tensor ggml_map_unary_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_unary_op_f32_t fun);

    public static native ggml_tensor ggml_map_binary_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_binary_op_f32_t fun);

    public static native ggml_tensor ggml_map_binary_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_binary_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom1_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_custom1_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom1_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_custom1_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom2_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_custom2_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom2_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_custom2_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom3_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                       ggml_custom3_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom3_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                       ggml_custom3_op_f32_t fun);
// Targeting ../ggml_custom1_op_t.java


// Targeting ../ggml_custom2_op_t.java


// Targeting ../ggml_custom3_op_t.java



    public static final int GGML_N_TASKS_MAX = -1;

    public static native ggml_tensor ggml_map_custom1(
                ggml_context ctx,
                ggml_tensor a,
                ggml_custom1_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom1_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_custom1_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom2(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_custom2_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom2_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_custom2_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom3(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                ggml_custom3_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom3_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                ggml_custom3_op_t fun,
                int n_tasks,
                Pointer userdata);

    // loss function

    public static native ggml_tensor ggml_cross_entropy_loss(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_cross_entropy_loss_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c);

    //
    // automatic differentiation
    //

    public static native void ggml_set_param(
                ggml_context ctx,
                ggml_tensor tensor);


    public static native void ggml_build_forward_expand(ggml_cgraph cgraph, ggml_tensor tensor);

    public static native @ByVal ggml_cgraph ggml_build_forward(ggml_tensor tensor);
    public static native @ByVal ggml_cgraph ggml_build_backward(ggml_context ctx, ggml_cgraph gf, @Cast("bool") boolean keep);

    // graph allocation in a context
    public static native ggml_cgraph ggml_new_graph(ggml_context ctx);
    public static native ggml_cgraph ggml_build_forward_ctx(ggml_context ctx, ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_graph_overhead();

    // ggml_graph_plan() has to be called before ggml_graph_compute()
    // when plan.work_size > 0, caller must allocate memory for plan.work_data
    public static native @ByVal ggml_cplan ggml_graph_plan(ggml_cgraph cgraph, int n_threads);
    public static native int ggml_graph_compute(ggml_cgraph cgraph, ggml_cplan cplan);
    public static native void ggml_graph_reset(ggml_cgraph cgraph);

    // same as ggml_graph_compute() but the work data is allocated as a part of the context
    // note: the drawback of this API is that you must have ensured that the context has enough memory for the work data
    public static native void ggml_graph_compute_with_ctx(ggml_context ctx, ggml_cgraph cgraph, int n_threads);

    public static native ggml_tensor ggml_graph_get_tensor(ggml_cgraph cgraph, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_graph_get_tensor(ggml_cgraph cgraph, String name);

    public static native void ggml_graph_export(@Const ggml_cgraph cgraph, @Cast("const char*") BytePointer fname);
    public static native void ggml_graph_export(@Const ggml_cgraph cgraph, String fname);
    public static native @ByVal ggml_cgraph ggml_graph_import(@Cast("const char*") BytePointer fname, @Cast("ggml_context**") PointerPointer ctx_data, @Cast("ggml_context**") PointerPointer ctx_eval);
    public static native @ByVal ggml_cgraph ggml_graph_import(@Cast("const char*") BytePointer fname, @ByPtrPtr ggml_context ctx_data, @ByPtrPtr ggml_context ctx_eval);
    public static native @ByVal ggml_cgraph ggml_graph_import(String fname, @ByPtrPtr ggml_context ctx_data, @ByPtrPtr ggml_context ctx_eval);

    // print info and performance information for the graph
    public static native void ggml_graph_print(@Const ggml_cgraph cgraph);

    // dump the graph into a file using the dot format
    public static native void ggml_graph_dump_dot(@Const ggml_cgraph gb, @Const ggml_cgraph gf, @Cast("const char*") BytePointer filename);
    public static native void ggml_graph_dump_dot(@Const ggml_cgraph gb, @Const ggml_cgraph gf, String filename);

    //
    // optimization
    //

    // optimization methods
    /** enum ggml_opt_type */
    public static final int
        GGML_OPT_ADAM = 0,
        GGML_OPT_LBFGS = 1;

    // linesearch methods
    /** enum ggml_linesearch */
    public static final int
        GGML_LINESEARCH_DEFAULT = 1,

        GGML_LINESEARCH_BACKTRACKING_ARMIJO       = 0,
        GGML_LINESEARCH_BACKTRACKING_WOLFE        = 1,
        GGML_LINESEARCH_BACKTRACKING_STRONG_WOLFE = 2;

    // optimization return values
    /** enum ggml_opt_result */
    public static final int
        GGML_OPT_OK = 0,
        GGML_OPT_DID_NOT_CONVERGE = 1,
        GGML_OPT_NO_CONTEXT = 2,
        GGML_OPT_INVALID_WOLFE = 3,
        GGML_OPT_FAIL = 4,

        GGML_LINESEARCH_FAIL = -128,
        GGML_LINESEARCH_MINIMUM_STEP = -127,
        GGML_LINESEARCH_MAXIMUM_STEP = -126,
        GGML_LINESEARCH_MAXIMUM_ITERATIONS = -125,
        GGML_LINESEARCH_INVALID_PARAMETERS = -124;
// Targeting ../ggml_opt_params.java


// Targeting ../ggml_opt_context.java



    public static native @ByVal ggml_opt_params ggml_opt_default_params(@Cast("ggml_opt_type") int type);

    // optimize the function defined by the tensor f
    public static native @Cast("ggml_opt_result") int ggml_opt(
                ggml_context ctx,
                @ByVal ggml_opt_params params,
                ggml_tensor f);

    // initialize optimizer context
    public static native void ggml_opt_init(
                ggml_context ctx,
                ggml_opt_context opt,
                @ByVal ggml_opt_params params,
                @Cast("int64_t") long nx);

    // continue optimizing the function defined by the tensor f
    public static native @Cast("ggml_opt_result") int ggml_opt_resume(
                ggml_context ctx,
                ggml_opt_context opt,
                ggml_tensor f);

    // continue optimizing the function defined by the tensor f
    public static native @Cast("ggml_opt_result") int ggml_opt_resume_g(
                ggml_context ctx,
                ggml_opt_context opt,
                ggml_tensor f,
                ggml_cgraph gf,
                ggml_cgraph gb);

    //
    // quantization
    //

    public static native @Cast("size_t") long ggml_quantize_q4_0(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q4_0(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q4_0(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
    public static native @Cast("size_t") long ggml_quantize_q4_1(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q4_1(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q4_1(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
    public static native @Cast("size_t") long ggml_quantize_q5_0(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q5_0(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q5_0(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
    public static native @Cast("size_t") long ggml_quantize_q5_1(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q5_1(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q5_1(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
    public static native @Cast("size_t") long ggml_quantize_q8_0(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q8_0(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q8_0(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);

    public static native @Cast("size_t") long ggml_quantize_chunk(@Cast("ggml_type") int type, @Const FloatPointer src, Pointer dst, int start, int n, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_chunk(@Cast("ggml_type") int type, @Const FloatBuffer src, Pointer dst, int start, int n, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_chunk(@Cast("ggml_type") int type, @Const float[] src, Pointer dst, int start, int n, @Cast("int64_t*") long[] hist);

    //
    // gguf
    //

    /** enum gguf_type */
    public static final int
        GGUF_TYPE_UINT8   = 0,
        GGUF_TYPE_INT8    = 1,
        GGUF_TYPE_UINT16  = 2,
        GGUF_TYPE_INT16   = 3,
        GGUF_TYPE_UINT32  = 4,
        GGUF_TYPE_INT32   = 5,
        GGUF_TYPE_FLOAT32 = 6,
        GGUF_TYPE_BOOL    = 7,
        GGUF_TYPE_STRING  = 8,
        GGUF_TYPE_ARRAY   = 9,
        GGUF_TYPE_COUNT = 10;       // marks the end of the enum
// Targeting ../gguf_context.java


// Targeting ../gguf_init_params.java



    public static native gguf_context gguf_init_empty();
    public static native gguf_context gguf_init_from_file(@Cast("const char*") BytePointer fname, @ByVal gguf_init_params params);
    public static native gguf_context gguf_init_from_file(String fname, @ByVal gguf_init_params params);
    //GGML_API struct gguf_context * gguf_init_from_buffer(..);

    public static native void gguf_free(gguf_context ctx);

    public static native @Cast("const char*") BytePointer gguf_type_name(@Cast("gguf_type") int type);

    public static native int gguf_get_version(gguf_context ctx);
    public static native @Cast("size_t") long gguf_get_alignment(gguf_context ctx);
    public static native @Cast("size_t") long gguf_get_data_offset(gguf_context ctx);
    public static native Pointer gguf_get_data(gguf_context ctx);

    public static native int gguf_get_n_kv(gguf_context ctx);
    public static native int gguf_find_key(gguf_context ctx, @Cast("const char*") BytePointer key);
    public static native int gguf_find_key(gguf_context ctx, String key);
    public static native @Cast("const char*") BytePointer gguf_get_key(gguf_context ctx, int i);

    public static native @Cast("gguf_type") int gguf_get_kv_type(gguf_context ctx, int i);
    public static native @Cast("gguf_type") int gguf_get_arr_type(gguf_context ctx, int i);

    // results are undefined if the wrong type is used for the key
    public static native @Cast("uint8_t") byte gguf_get_val_u8(gguf_context ctx, int i);
    public static native byte gguf_get_val_i8(gguf_context ctx, int i);
    public static native @Cast("uint16_t") short gguf_get_val_u16(gguf_context ctx, int i);
    public static native short gguf_get_val_i16(gguf_context ctx, int i);
    public static native @Cast("uint32_t") int gguf_get_val_u32(gguf_context ctx, int i);
    public static native int gguf_get_val_i32(gguf_context ctx, int i);
    public static native float gguf_get_val_f32(gguf_context ctx, int i);
    public static native @Cast("bool") boolean gguf_get_val_bool(gguf_context ctx, int i);
    public static native @Cast("const char*") BytePointer gguf_get_val_str(gguf_context ctx, int i);
    public static native int gguf_get_arr_n(gguf_context ctx, int i);
    public static native @Const Pointer gguf_get_arr_data(gguf_context ctx, int i);
    public static native @Cast("const char*") BytePointer gguf_get_arr_str(gguf_context ctx, int key_id, int i);

    public static native int gguf_get_n_tensors(gguf_context ctx);
    public static native int gguf_find_tensor(gguf_context ctx, @Cast("const char*") BytePointer name);
    public static native int gguf_find_tensor(gguf_context ctx, String name);
    public static native @Cast("size_t") long gguf_get_tensor_offset(gguf_context ctx, int i);
    public static native @Cast("char*") BytePointer gguf_get_tensor_name(gguf_context ctx, int i);

    // overrides existing values or adds a new one
    public static native void gguf_set_val_u8(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("uint8_t") byte val);
    public static native void gguf_set_val_u8(gguf_context ctx, String key, @Cast("uint8_t") byte val);
    public static native void gguf_set_val_i8(gguf_context ctx, @Cast("const char*") BytePointer key, byte val);
    public static native void gguf_set_val_i8(gguf_context ctx, String key, byte val);
    public static native void gguf_set_val_u16(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("uint16_t") short val);
    public static native void gguf_set_val_u16(gguf_context ctx, String key, @Cast("uint16_t") short val);
    public static native void gguf_set_val_i16(gguf_context ctx, @Cast("const char*") BytePointer key, short val);
    public static native void gguf_set_val_i16(gguf_context ctx, String key, short val);
    public static native void gguf_set_val_u32(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("uint32_t") int val);
    public static native void gguf_set_val_u32(gguf_context ctx, String key, @Cast("uint32_t") int val);
    public static native void gguf_set_val_i32(gguf_context ctx, @Cast("const char*") BytePointer key, int val);
    public static native void gguf_set_val_i32(gguf_context ctx, String key, int val);
    public static native void gguf_set_val_f32(gguf_context ctx, @Cast("const char*") BytePointer key, float val);
    public static native void gguf_set_val_f32(gguf_context ctx, String key, float val);
    public static native void gguf_set_val_bool(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("bool") boolean val);
    public static native void gguf_set_val_bool(gguf_context ctx, String key, @Cast("bool") boolean val);
    public static native void gguf_set_val_str(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("const char*") BytePointer val);
    public static native void gguf_set_val_str(gguf_context ctx, String key, String val);
    public static native void gguf_set_arr_data(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("gguf_type") int type, @Const Pointer data, int n);
    public static native void gguf_set_arr_data(gguf_context ctx, String key, @Cast("gguf_type") int type, @Const Pointer data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("const char**") PointerPointer data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("const char**") @ByPtrPtr BytePointer data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, String key, @Cast("const char**") @ByPtrPtr ByteBuffer data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("const char**") @ByPtrPtr byte[] data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, String key, @Cast("const char**") @ByPtrPtr BytePointer data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("const char**") @ByPtrPtr ByteBuffer data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, String key, @Cast("const char**") @ByPtrPtr byte[] data, int n);

    // set or add KV pairs from another context
    public static native void gguf_set_kv(gguf_context ctx, gguf_context src);

    // manage tensor info
    public static native void gguf_add_tensor(gguf_context ctx, @Const ggml_tensor tensor);
    public static native void gguf_set_tensor_type(gguf_context ctx, @Cast("const char*") BytePointer name, @Cast("ggml_type") int type);
    public static native void gguf_set_tensor_type(gguf_context ctx, String name, @Cast("ggml_type") int type);
    public static native void gguf_set_tensor_data(gguf_context ctx, @Cast("const char*") BytePointer name, @Const Pointer data, @Cast("size_t") long size);
    public static native void gguf_set_tensor_data(gguf_context ctx, String name, @Const Pointer data, @Cast("size_t") long size);

    // writing gguf files can be done in 2 ways:
    //
    // - write the entire gguf_context to a binary file in a single pass:
    //
    //   gguf_write_to_file(ctx, fname);
    //
    // - first prepare a file with a placeholder for the meta data, write the tensor data, then write the meta data:
    //
    //   FILE * f = fopen(fname, "wb");
    //   fseek(f, gguf_get_meta_size(ctx), SEEK_SET);
    //   fwrite(f, ...);
    //   void * data = gguf_meta_get_meta_data(ctx);
    //   fseek(f, 0, SEEK_SET);
    //   fwrite(f, data, gguf_get_meta_size(ctx));
    //   free(data);
    //   fclose(f);
    //

    // write the entire context to a binary file
    public static native void gguf_write_to_file(gguf_context ctx, @Cast("const char*") BytePointer fname, @Cast("bool") boolean only_meta);
    public static native void gguf_write_to_file(gguf_context ctx, String fname, @Cast("bool") boolean only_meta);

    // get the size in bytes of the meta data (header, kv pairs, tensor info) including padding
    public static native @Cast("size_t") long gguf_get_meta_size(gguf_context ctx);
    public static native void gguf_get_meta_data(gguf_context ctx, Pointer data);

    //
    // system info
    //

    public static native int ggml_cpu_has_avx();
    public static native int ggml_cpu_has_avx2();
    public static native int ggml_cpu_has_avx512();
    public static native int ggml_cpu_has_avx512_vbmi();
    public static native int ggml_cpu_has_avx512_vnni();
    public static native int ggml_cpu_has_fma();
    public static native int ggml_cpu_has_neon();
    public static native int ggml_cpu_has_arm_fma();
    public static native int ggml_cpu_has_f16c();
    public static native int ggml_cpu_has_fp16_va();
    public static native int ggml_cpu_has_wasm_simd();
    public static native int ggml_cpu_has_blas();
    public static native int ggml_cpu_has_cublas();
    public static native int ggml_cpu_has_clblast();
    public static native int ggml_cpu_has_gpublas();
    public static native int ggml_cpu_has_sse3();
    public static native int ggml_cpu_has_vsx();

    //
    // Internal types and functions exposed for tests and benchmarks
    //

// #ifdef  __cplusplus
// restrict not standard in C++
// #define GGML_RESTRICT
// #else
// #define GGML_RESTRICT restrict
// Targeting ../ggml_to_float_t.java


// Targeting ../ggml_from_float_t.java


// Targeting ../ggml_vec_dot_t.java


// Targeting ../ggml_type_traits_t.java



    public static native @ByVal ggml_type_traits_t ggml_internal_get_type_traits(@Cast("ggml_type") int type);

// #ifdef  __cplusplus
// #endif


// Parsed from k_quants.h

// #pragma once

// #include "ggml.h"

// #include <stdint.h>
// #include <assert.h>
// #include <stddef.h>

// Super-block size
// #ifdef GGML_QKK_64
// #else
public static final int QK_K = 256;
public static final int K_SCALE_SIZE = 12;
// #endif

// #ifndef static_assert
// #if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 201100L)
// #define static_assert(cond, msg) _Static_assert(cond, msg)
// #else
// #define static_assert(cond, msg) struct global_scope_noop_trick
// Targeting ../block_q2_K.java


// Targeting ../block_q3_K.java


// Targeting ../block_q4_K.java


// Targeting ../block_q5_K.java


// Targeting ../block_q6_K.java


// Targeting ../block_q8_K.java




// Quantization
public static native void quantize_row_q2_K_reference(@Const FloatPointer x, block_q2_K y, int k);
public static native void quantize_row_q2_K_reference(@Const FloatBuffer x, block_q2_K y, int k);
public static native void quantize_row_q2_K_reference(@Const float[] x, block_q2_K y, int k);
public static native void quantize_row_q3_K_reference(@Const FloatPointer x, block_q3_K y, int k);
public static native void quantize_row_q3_K_reference(@Const FloatBuffer x, block_q3_K y, int k);
public static native void quantize_row_q3_K_reference(@Const float[] x, block_q3_K y, int k);
public static native void quantize_row_q4_K_reference(@Const FloatPointer x, block_q4_K y, int k);
public static native void quantize_row_q4_K_reference(@Const FloatBuffer x, block_q4_K y, int k);
public static native void quantize_row_q4_K_reference(@Const float[] x, block_q4_K y, int k);
public static native void quantize_row_q5_K_reference(@Const FloatPointer x, block_q5_K y, int k);
public static native void quantize_row_q5_K_reference(@Const FloatBuffer x, block_q5_K y, int k);
public static native void quantize_row_q5_K_reference(@Const float[] x, block_q5_K y, int k);
public static native void quantize_row_q6_K_reference(@Const FloatPointer x, block_q6_K y, int k);
public static native void quantize_row_q6_K_reference(@Const FloatBuffer x, block_q6_K y, int k);
public static native void quantize_row_q6_K_reference(@Const float[] x, block_q6_K y, int k);
public static native void quantize_row_q8_K_reference(@Const FloatPointer x, block_q8_K y, int k);
public static native void quantize_row_q8_K_reference(@Const FloatBuffer x, block_q8_K y, int k);
public static native void quantize_row_q8_K_reference(@Const float[] x, block_q8_K y, int k);

public static native void quantize_row_q2_K(@Const FloatPointer x, Pointer y, int k);
public static native void quantize_row_q2_K(@Const FloatBuffer x, Pointer y, int k);
public static native void quantize_row_q2_K(@Const float[] x, Pointer y, int k);
public static native void quantize_row_q3_K(@Const FloatPointer x, Pointer y, int k);
public static native void quantize_row_q3_K(@Const FloatBuffer x, Pointer y, int k);
public static native void quantize_row_q3_K(@Const float[] x, Pointer y, int k);
public static native void quantize_row_q4_K(@Const FloatPointer x, Pointer y, int k);
public static native void quantize_row_q4_K(@Const FloatBuffer x, Pointer y, int k);
public static native void quantize_row_q4_K(@Const float[] x, Pointer y, int k);
public static native void quantize_row_q5_K(@Const FloatPointer x, Pointer y, int k);
public static native void quantize_row_q5_K(@Const FloatBuffer x, Pointer y, int k);
public static native void quantize_row_q5_K(@Const float[] x, Pointer y, int k);
public static native void quantize_row_q6_K(@Const FloatPointer x, Pointer y, int k);
public static native void quantize_row_q6_K(@Const FloatBuffer x, Pointer y, int k);
public static native void quantize_row_q6_K(@Const float[] x, Pointer y, int k);
public static native void quantize_row_q8_K(@Const FloatPointer x, Pointer y, int k);
public static native void quantize_row_q8_K(@Const FloatBuffer x, Pointer y, int k);
public static native void quantize_row_q8_K(@Const float[] x, Pointer y, int k);

// Dequantization
public static native void dequantize_row_q2_K(@Const block_q2_K x, FloatPointer y, int k);
public static native void dequantize_row_q2_K(@Const block_q2_K x, FloatBuffer y, int k);
public static native void dequantize_row_q2_K(@Const block_q2_K x, float[] y, int k);
public static native void dequantize_row_q3_K(@Const block_q3_K x, FloatPointer y, int k);
public static native void dequantize_row_q3_K(@Const block_q3_K x, FloatBuffer y, int k);
public static native void dequantize_row_q3_K(@Const block_q3_K x, float[] y, int k);
public static native void dequantize_row_q4_K(@Const block_q4_K x, FloatPointer y, int k);
public static native void dequantize_row_q4_K(@Const block_q4_K x, FloatBuffer y, int k);
public static native void dequantize_row_q4_K(@Const block_q4_K x, float[] y, int k);
public static native void dequantize_row_q5_K(@Const block_q5_K x, FloatPointer y, int k);
public static native void dequantize_row_q5_K(@Const block_q5_K x, FloatBuffer y, int k);
public static native void dequantize_row_q5_K(@Const block_q5_K x, float[] y, int k);
public static native void dequantize_row_q6_K(@Const block_q6_K x, FloatPointer y, int k);
public static native void dequantize_row_q6_K(@Const block_q6_K x, FloatBuffer y, int k);
public static native void dequantize_row_q6_K(@Const block_q6_K x, float[] y, int k);
public static native void dequantize_row_q8_K(@Const block_q8_K x, FloatPointer y, int k);
public static native void dequantize_row_q8_K(@Const block_q8_K x, FloatBuffer y, int k);
public static native void dequantize_row_q8_K(@Const block_q8_K x, float[] y, int k);

// Dot product
public static native void ggml_vec_dot_q2_K_q8_K(int n, FloatPointer s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q2_K_q8_K(int n, FloatBuffer s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q2_K_q8_K(int n, float[] s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q3_K_q8_K(int n, FloatPointer s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q3_K_q8_K(int n, FloatBuffer s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q3_K_q8_K(int n, float[] s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q4_K_q8_K(int n, FloatPointer s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q4_K_q8_K(int n, FloatBuffer s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q4_K_q8_K(int n, float[] s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q5_K_q8_K(int n, FloatPointer s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q5_K_q8_K(int n, FloatBuffer s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q5_K_q8_K(int n, float[] s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q6_K_q8_K(int n, FloatPointer s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q6_K_q8_K(int n, FloatBuffer s, @Const Pointer vx, @Const Pointer vy);
public static native void ggml_vec_dot_q6_K_q8_K(int n, float[] s, @Const Pointer vx, @Const Pointer vy);

// Quantization with histogram collection
public static native @Cast("size_t") long ggml_quantize_q2_K(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
public static native @Cast("size_t") long ggml_quantize_q2_K(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
public static native @Cast("size_t") long ggml_quantize_q2_K(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
public static native @Cast("size_t") long ggml_quantize_q3_K(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
public static native @Cast("size_t") long ggml_quantize_q3_K(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
public static native @Cast("size_t") long ggml_quantize_q3_K(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
public static native @Cast("size_t") long ggml_quantize_q4_K(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
public static native @Cast("size_t") long ggml_quantize_q4_K(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
public static native @Cast("size_t") long ggml_quantize_q4_K(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
public static native @Cast("size_t") long ggml_quantize_q5_K(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
public static native @Cast("size_t") long ggml_quantize_q5_K(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
public static native @Cast("size_t") long ggml_quantize_q5_K(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
public static native @Cast("size_t") long ggml_quantize_q6_K(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
public static native @Cast("size_t") long ggml_quantize_q6_K(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
public static native @Cast("size_t") long ggml_quantize_q6_K(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);



}
