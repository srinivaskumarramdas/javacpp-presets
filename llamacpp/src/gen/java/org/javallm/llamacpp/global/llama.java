// Targeted by JavaCPP version 1.5.10-SNAPSHOT: DO NOT EDIT THIS FILE

package org.javallm.llamacpp.global;

import org.javallm.llamacpp.*;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

public class llama extends org.javallm.llamacpp.presets.llama {
    static { Loader.load(); }

// Parsed from ggml.h

// #pragma once

//
// GGML Tensor Library
//
// This documentation is still a work in progress.
// If you wish some specific topics to be covered, feel free to drop a comment:
//
//   https://github.com/ggerganov/whisper.cpp/issues/40
//
// ## Overview
//
// This library implements:
//
//  - a set of tensor operations
//  - automatic differentiation
//  - basic optimization algorithms
//
// The aim of this library is to provide a minimalistic approach for various machine learning tasks. This includes,
// but is not limited to, the following:
//
//  - linear regression
//  - support vector machines
//  - neural networks
//
// The library allows the user to define a certain function using the available tensor operations. This function
// definition is represented internally via a computation graph. Each tensor operation in the function definition
// corresponds to a node in the graph. Having the computation graph defined, the user can choose to compute the
// function's value and/or its gradient with respect to the input variables. Optionally, the function can be optimized
// using one of the available optimization algorithms.
//
// For example, here we define the function: f(x) = a*x^2 + b
//
//   {
//       struct ggml_init_params params = {
//           .mem_size   = 16*1024*1024,
//           .mem_buffer = NULL,
//       };
//
//       // memory allocation happens here
//       struct ggml_context * ctx = ggml_init(params);
//
//       struct ggml_tensor * x = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//
//       ggml_set_param(ctx, x); // x is an input variable
//
//       struct ggml_tensor * a  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//       struct ggml_tensor * b  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//       struct ggml_tensor * x2 = ggml_mul(ctx, x, x);
//       struct ggml_tensor * f  = ggml_add(ctx, ggml_mul(ctx, a, x2), b);
//
//       ...
//   }
//
// Notice that the function definition above does not involve any actual computation. The computation is performed only
// when the user explicitly requests it. For example, to compute the function's value at x = 2.0:
//
//   {
//       ...
//
//       struct ggml_cgraph gf = ggml_build_forward(f);
//
//       // set the input variable and parameter values
//       ggml_set_f32(x, 2.0f);
//       ggml_set_f32(a, 3.0f);
//       ggml_set_f32(b, 4.0f);
//
//       ggml_graph_compute_with_ctx(ctx, &gf, n_threads);
//
//       printf("f = %f\n", ggml_get_f32_1d(f, 0));
//
//       ...
//   }
//
// The actual computation is performed in the ggml_graph_compute() function.
//
// The ggml_new_tensor_...() functions create new tensors. They are allocated in the memory buffer provided to the
// ggml_init() function. You have to be careful not to exceed the memory buffer size. Therefore, you have to know
// in advance how much memory you need for your computation. Alternatively, you can allocate a large enough memory
// and after defining the computation graph, call the ggml_used_mem() function to find out how much memory was
// actually needed.
//
// The ggml_set_param() function marks a tensor as an input variable. This is used by the automatic
// differentiation and optimization algorithms.
//
// The described approach allows to define the function graph once and then compute its forward or backward graphs
// multiple times. All computations will use the same memory buffer allocated in the ggml_init() function. This way
// the user can avoid the memory allocation overhead at runtime.
//
// The library supports multi-dimensional tensors - up to 4 dimensions. The FP16 and FP32 data types are first class
// citizens, but in theory the library can be extended to support FP8 and integer data types.
//
// Each tensor operation produces a new tensor. Initially the library was envisioned to support only the use of unary
// and binary operations. Most of the available operations fall into one of these two categories. With time, it became
// clear that the library needs to support more complex operations. The way to support these operations is not clear
// yet, but a few examples are demonstrated in the following operations:
//
//   - ggml_permute()
//   - ggml_conv_1d_1s()
//   - ggml_conv_1d_2s()
//
// For each tensor operator, the library implements a forward and backward computation function. The forward function
// computes the output tensor value given the input tensor values. The backward function computes the adjoint of the
// input tensors given the adjoint of the output tensor. For a detailed explanation of what this means, take a
// calculus class, or watch the following video:
//
//   What is Automatic Differentiation?
//   https://www.youtube.com/watch?v=wG_nF1awSSY
//
//
// ## Tensor data (struct ggml_tensor)
//
// The tensors are stored in memory via the ggml_tensor struct. The structure provides information about the size of
// the tensor, the data type, and the memory buffer where the tensor data is stored. Additionally, it contains
// pointers to the "source" tensors - i.e. the tensors that were used to compute the current tensor. For example:
//
//   {
//       struct ggml_tensor * c = ggml_add(ctx, a, b);
//
//       assert(c->src[0] == a);
//       assert(c->src[1] == b);
//   }
//
// The multi-dimensional tensors are stored in row-major order. The ggml_tensor struct contains fields for the
// number of elements in each dimension ("ne") as well as the number of bytes ("nb", a.k.a. stride). This allows
// to store tensors that are not contiguous in memory, which is useful for operations such as transposition and
// permutation. All tensor operations have to take the stride into account and not assume that the tensor is
// contiguous in memory.
//
// The data of the tensor is accessed via the "data" pointer. For example:
//
//   {
//       struct ggml_tensor * a = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, 2, 3);
//
//       // a[2, 1] = 1.0f;
//       *(float *) ((char *) a->data + 2*a->nb[1] + 1*a->nb[0]) = 1.0f;
//
//       // a[0, 2] = 2.0f;
//       *(float *) ((char *) a->data + 0*a->nb[1] + 2*a->nb[0]) = 2.0f;
//
//       ...
//   }
//
// Alternatively, there are helper functions, such as ggml_get_f32_1d() and ggml_set_f32_1d() that can be used.
//
// ## The matrix multiplication operator (ggml_mul_mat)
//
// TODO
//
//
// ## Multi-threading
//
// TODO
//
//
// ## Overview of ggml.c
//
// TODO
//
//
// ## SIMD optimizations
//
// TODO
//
//
// ## Debugging ggml
//
// TODO
//
//

// #ifdef GGML_SHARED
// #else
// #    define GGML_API
// #endif

// #include <stdint.h>
// #include <stddef.h>
// #include <stdbool.h>

public static final int GGML_FILE_MAGIC =   0x67676d6c; // "ggml"
public static final int GGML_FILE_VERSION = 1;

public static final int GGML_QNT_VERSION =        2;    // bump this on quantization format changes
public static final int GGML_QNT_VERSION_FACTOR = 1000; // do not change this

public static final int GGML_MAX_DIMS =          4;
public static final int GGML_MAX_NODES =         4096;
public static final int GGML_MAX_PARAMS =        256;
public static final int GGML_MAX_CONTEXTS =      64;
public static final int GGML_MAX_SRC =           6;
public static final int GGML_MAX_NAME =          48;
public static final int GGML_DEFAULT_N_THREADS = 4;


public static final int GGML_EXIT_SUCCESS = 0;
public static final int GGML_EXIT_ABORTED = 1;

// #define GGML_UNUSED(x) (void)(x)


// #define GGML_ASSERT(x)
//     do {
//         if (!(x)) {
//             fprintf(stderr, "GGML_ASSERT: %s:%d: %s\n", __FILE__, __LINE__, #x);
//             abort();
//         }
//     } while (0)

// used to copy the number of elements and stride in bytes of tensors into local variables.
// main purpose is to reduce code duplication and improve readability.
//
// example:
//
//    GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne);
//    GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb);
//
// #define GGML_TENSOR_LOCALS_1(type, prefix, pointer, array)
//     const type prefix##0 = (pointer)->array[0];
//     GGML_UNUSED(prefix##0);
// #define GGML_TENSOR_LOCALS_2(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_1    (type, prefix, pointer, array)
//     const type prefix##1 = (pointer)->array[1];
//     GGML_UNUSED(prefix##1);
// #define GGML_TENSOR_LOCALS_3(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_2    (type, prefix, pointer, array)
//     const type prefix##2 = (pointer)->array[2];
//     GGML_UNUSED(prefix##2);
// #define GGML_TENSOR_LOCALS(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_3  (type, prefix, pointer, array)
//     const type prefix##3 = (pointer)->array[3];
//     GGML_UNUSED(prefix##3);

// #ifdef  __cplusplus
// #endif

// #ifdef __ARM_NEON
// #else
// #endif

    // convert FP16 <-> FP32
    public static native float ggml_fp16_to_fp32(@Cast("ggml_fp16_t") short x);
    public static native @Cast("ggml_fp16_t") short ggml_fp32_to_fp16(float x);

    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") ShortPointer x, FloatPointer y, int n);
    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") ShortBuffer x, FloatBuffer y, int n);
    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") short[] x, float[] y, int n);
    public static native void ggml_fp32_to_fp16_row(@Const FloatPointer x, @Cast("ggml_fp16_t*") ShortPointer y, int n);
    public static native void ggml_fp32_to_fp16_row(@Const FloatBuffer x, @Cast("ggml_fp16_t*") ShortBuffer y, int n);
    public static native void ggml_fp32_to_fp16_row(@Const float[] x, @Cast("ggml_fp16_t*") short[] y, int n);
// Targeting ../ggml_context.java



    /** enum ggml_type */
    public static final int
        GGML_TYPE_F32  = 0,
        GGML_TYPE_F16  = 1,
        GGML_TYPE_Q4_0 = 2,
        GGML_TYPE_Q4_1 = 3,
        // GGML_TYPE_Q4_2 = 4, support has been removed
        // GGML_TYPE_Q4_3 (5) support has been removed
        GGML_TYPE_Q5_0 = 6,
        GGML_TYPE_Q5_1 = 7,
        GGML_TYPE_Q8_0 = 8,
        GGML_TYPE_Q8_1 = 9,
        // k-quantizations
        GGML_TYPE_Q2_K = 10,
        GGML_TYPE_Q3_K = 11,
        GGML_TYPE_Q4_K = 12,
        GGML_TYPE_Q5_K = 13,
        GGML_TYPE_Q6_K = 14,
        GGML_TYPE_Q8_K = 15,
        GGML_TYPE_I8 = 16,
        GGML_TYPE_I16 = 17,
        GGML_TYPE_I32 = 18,
        GGML_TYPE_COUNT = 19;

    /** enum ggml_backend */
    public static final int
        GGML_BACKEND_CPU = 0,
        GGML_BACKEND_GPU = 10,
        GGML_BACKEND_GPU_SPLIT = 20;

    // model file types
    /** enum ggml_ftype */
    public static final int
        GGML_FTYPE_UNKNOWN     = -1,
        GGML_FTYPE_ALL_F32     = 0,
        GGML_FTYPE_MOSTLY_F16  = 1,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_0 = 2,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_1 = 3,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4, // tok_embeddings.weight and output.weight are F16
        GGML_FTYPE_MOSTLY_Q8_0 = 7,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_0 = 8,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_1 = 9,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q2_K = 10, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q3_K = 11, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_K = 12, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_K = 13, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q6_K = 14; // except 1d tensors

    // available tensor operations:
    /** enum ggml_op */
    public static final int
        GGML_OP_NONE = 0,

        GGML_OP_DUP = 1,
        GGML_OP_ADD = 2,
        GGML_OP_ADD1 = 3,
        GGML_OP_ACC = 4,
        GGML_OP_SUB = 5,
        GGML_OP_MUL = 6,
        GGML_OP_DIV = 7,
        GGML_OP_SQR = 8,
        GGML_OP_SQRT = 9,
        GGML_OP_LOG = 10,
        GGML_OP_SUM = 11,
        GGML_OP_SUM_ROWS = 12,
        GGML_OP_MEAN = 13,
        GGML_OP_ARGMAX = 14,
        GGML_OP_REPEAT = 15,
        GGML_OP_REPEAT_BACK = 16,
        GGML_OP_ABS = 17,
        GGML_OP_SGN = 18,
        GGML_OP_NEG = 19,
        GGML_OP_STEP = 20,
        GGML_OP_TANH = 21,
        GGML_OP_ELU = 22,
        GGML_OP_RELU = 23,
        GGML_OP_GELU = 24,
        GGML_OP_GELU_QUICK = 25,
        GGML_OP_SILU = 26,
        GGML_OP_SILU_BACK = 27,
        GGML_OP_NORM = 28, // normalize
        GGML_OP_RMS_NORM = 29,
        GGML_OP_RMS_NORM_BACK = 30,

        GGML_OP_MUL_MAT = 31,
        GGML_OP_OUT_PROD = 32,

        GGML_OP_SCALE = 33,
        GGML_OP_SET = 34,
        GGML_OP_CPY = 35,
        GGML_OP_CONT = 36,
        GGML_OP_RESHAPE = 37,
        GGML_OP_VIEW = 38,
        GGML_OP_PERMUTE = 39,
        GGML_OP_TRANSPOSE = 40,
        GGML_OP_GET_ROWS = 41,
        GGML_OP_GET_ROWS_BACK = 42,
        GGML_OP_DIAG = 43,
        GGML_OP_DIAG_MASK_INF = 44,
        GGML_OP_DIAG_MASK_ZERO = 45,
        GGML_OP_SOFT_MAX = 46,
        GGML_OP_SOFT_MAX_BACK = 47,
        GGML_OP_ROPE = 48,
        GGML_OP_ROPE_BACK = 49,
        GGML_OP_ALIBI = 50,
        GGML_OP_CLAMP = 51,
        GGML_OP_CONV_1D = 52,
        GGML_OP_CONV_2D = 53,

        GGML_OP_FLASH_ATTN = 54,
        GGML_OP_FLASH_FF = 55,
        GGML_OP_FLASH_ATTN_BACK = 56,
        GGML_OP_WIN_PART = 57,
        GGML_OP_WIN_UNPART = 58,

        GGML_OP_MAP_UNARY = 59,
        GGML_OP_MAP_BINARY = 60,

        GGML_OP_MAP_CUSTOM1 = 61,
        GGML_OP_MAP_CUSTOM2 = 62,
        GGML_OP_MAP_CUSTOM3 = 63,

        GGML_OP_CROSS_ENTROPY_LOSS = 64,
        GGML_OP_CROSS_ENTROPY_LOSS_BACK = 65,

        GGML_OP_COUNT = 66;
// Targeting ../ggml_object.java



    @MemberGetter public static native @Cast("const size_t") long GGML_OBJECT_SIZE();
    public static final long GGML_OBJECT_SIZE = GGML_OBJECT_SIZE();
// Targeting ../ggml_tensor.java



    @MemberGetter public static native @Cast("const size_t") long GGML_TENSOR_SIZE();
    public static final long GGML_TENSOR_SIZE = GGML_TENSOR_SIZE();
// Targeting ../ggml_cplan.java


// Targeting ../ggml_cgraph.java


// Targeting ../ggml_scratch.java


// Targeting ../ggml_init_params.java




    // compute types

    // NOTE: the INIT or FINALIZE pass is not scheduled unless explicitly enabled.
    // This behavior was changed since https://github.com/ggerganov/llama.cpp/pull/1995.
    /** enum ggml_task_type */
    public static final int
        GGML_TASK_INIT = 0,
        GGML_TASK_COMPUTE = 1,
        GGML_TASK_FINALIZE = 2;
// Targeting ../ggml_compute_params.java



    // misc

    public static native void ggml_time_init(); // call this once at the beginning of the program
    public static native @Cast("int64_t") long ggml_time_ms();
    public static native @Cast("int64_t") long ggml_time_us();
    public static native @Cast("int64_t") long ggml_cycles();
    public static native @Cast("int64_t") long ggml_cycles_per_ms();

    public static native void ggml_numa_init(); // call once for better performance on NUMA systems
    public static native @Cast("bool") boolean ggml_is_numa(); // true if init detected that system has >1 NUMA node

    public static native void ggml_print_object(@Const ggml_object obj);
    public static native void ggml_print_objects(@Const ggml_context ctx);

    public static native @Cast("int64_t") long ggml_nelements(@Const ggml_tensor tensor);
    public static native @Cast("int64_t") long ggml_nrows(@Const ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_nbytes(@Const ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_nbytes_split(@Const ggml_tensor tensor, int nrows_split);

    public static native int ggml_blck_size(@Cast("ggml_type") int type);
    public static native @Cast("size_t") long ggml_type_size(@Cast("ggml_type") int type); // size in bytes for all elements in a block
    public static native float ggml_type_sizef(@Cast("ggml_type") int type); // ggml_type_size()/ggml_blck_size() as float

    public static native @Cast("const char*") BytePointer ggml_type_name(@Cast("ggml_type") int type);
    public static native @Cast("const char*") BytePointer ggml_op_name(@Cast("ggml_op") int op);

    public static native @Cast("size_t") long ggml_element_size(@Const ggml_tensor tensor);

    public static native @Cast("bool") boolean ggml_is_quantized(@Cast("ggml_type") int type);

    // TODO: temporary until model loading of ggml examples is refactored
    public static native @Cast("ggml_type") int ggml_ftype_to_ggml_type(@Cast("ggml_ftype") int ftype);

    public static native @Cast("bool") boolean ggml_is_transposed(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_contiguous(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_permuted(@Const ggml_tensor tensor);

    // use this to compute the memory overhead of a tensor
    public static native @Cast("size_t") long ggml_tensor_overhead();

    // main

    public static native ggml_context ggml_init(@ByVal ggml_init_params params);
    public static native void ggml_free(ggml_context ctx);

    public static native @Cast("size_t") long ggml_used_mem(@Const ggml_context ctx);

    public static native @Cast("size_t") long ggml_set_scratch(ggml_context ctx, @ByVal ggml_scratch scratch);
    public static native void ggml_set_no_alloc(ggml_context ctx, @Cast("bool") boolean no_alloc);

    public static native Pointer ggml_get_mem_buffer(@Const ggml_context ctx);
    public static native @Cast("size_t") long ggml_get_mem_size(@Const ggml_context ctx);
    public static native @Cast("size_t") long ggml_get_max_tensor_size(@Const ggml_context ctx);

    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") LongPointer ne);
    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") LongBuffer ne);
    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") long[] ne);

    public static native ggml_tensor ggml_new_tensor_1d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0);

    public static native ggml_tensor ggml_new_tensor_2d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1);

    public static native ggml_tensor ggml_new_tensor_3d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2);

    public static native ggml_tensor ggml_new_tensor_4d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3);

    public static native ggml_tensor ggml_new_i32(ggml_context ctx, int value);
    public static native ggml_tensor ggml_new_f32(ggml_context ctx, float value);

    public static native ggml_tensor ggml_dup_tensor(ggml_context ctx, @Const ggml_tensor src);
    public static native ggml_tensor ggml_view_tensor(ggml_context ctx, @Const ggml_tensor src);

    public static native ggml_tensor ggml_get_tensor(ggml_context ctx, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_get_tensor(ggml_context ctx, String name);

    public static native ggml_tensor ggml_set_zero(ggml_tensor tensor);
    public static native ggml_tensor ggml_set_i32(ggml_tensor tensor, int value);
    public static native ggml_tensor ggml_set_f32(ggml_tensor tensor, float value);

    public static native int ggml_get_i32_1d(@Const ggml_tensor tensor, int i);
    public static native void ggml_set_i32_1d(@Const ggml_tensor tensor, int i, int value);

    public static native float ggml_get_f32_1d(@Const ggml_tensor tensor, int i);
    public static native void ggml_set_f32_1d(@Const ggml_tensor tensor, int i, float value);

    public static native Pointer ggml_get_data(@Const ggml_tensor tensor);
    public static native FloatPointer ggml_get_data_f32(@Const ggml_tensor tensor);

    public static native @Cast("const char*") BytePointer ggml_get_name(@Const ggml_tensor tensor);
    public static native ggml_tensor ggml_set_name(ggml_tensor tensor, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_set_name(ggml_tensor tensor, String name);
    public static native ggml_tensor ggml_format_name(ggml_tensor tensor, @Cast("const char*") BytePointer fmt);
    public static native ggml_tensor ggml_format_name(ggml_tensor tensor, String fmt);

    //
    // operations on tensors with backpropagation
    //

    public static native ggml_tensor ggml_dup(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_add(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add1(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add1_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_acc(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_acc_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_sub(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_sub_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_mul(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_mul_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_div(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_div_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_sqr(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqr_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqrt(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqrt_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_log(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_log_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // return scalar
    public static native ggml_tensor ggml_sum(
                ggml_context ctx,
                ggml_tensor a);

    // sums along rows, with input shape [a,b,c,d] return shape [1,b,c,d]
    public static native ggml_tensor ggml_sum_rows(
                ggml_context ctx,
                ggml_tensor a);

    // mean along rows
    public static native ggml_tensor ggml_mean(
                ggml_context ctx,
                ggml_tensor a);

    // argmax along rows
    public static native ggml_tensor ggml_argmax(
                ggml_context ctx,
                ggml_tensor a);

    // if a is the same shape as b, and a is not parameter, return a
    // otherwise, return a new tensor: repeat(a) to fit in b
    public static native ggml_tensor ggml_repeat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_repeat_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_abs(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_abs_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sgn(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sgn_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_neg(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_neg_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_step(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_step_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_tanh(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_tanh_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_elu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_elu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_relu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_relu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // TODO: double-check this computation is correct
    public static native ggml_tensor ggml_gelu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_quick(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_quick_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_silu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_silu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // a - x
    // b - dy
    public static native ggml_tensor ggml_silu_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // normalize along rows
    // TODO: eps is hardcoded to 1e-5 for now
    public static native ggml_tensor ggml_norm(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_norm_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_rms_norm(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_rms_norm_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // a - x
    // b - dy
    public static native ggml_tensor ggml_rms_norm_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // A: n columns, m rows
    // B: n columns, p rows  (i.e. we transpose it internally)
    // result is m columns, p rows
    public static native ggml_tensor ggml_mul_mat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // A: m columns, n rows,
    // B: p columns, n rows,
    // result is m columns, p rows
    public static native ggml_tensor ggml_out_prod(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    //
    // operations on tensors without backpropagation
    //

    public static native ggml_tensor ggml_scale(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_scale_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // b -> view(a,offset,nb1,nb2,3), return modified a
    public static native ggml_tensor ggml_set(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    // b -> view(a,offset,nb1,nb2,3), return view(a)
    public static native ggml_tensor ggml_set_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_set_1d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_set_1d_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long offset);

    // b -> view(a,offset,nb1,nb2,3), return modified a
    public static native ggml_tensor ggml_set_2d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset);

    // b -> view(a,offset,nb1,nb2,3), return view(a)
    public static native ggml_tensor ggml_set_2d_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset);


    // a -> b, return view(b)
    public static native ggml_tensor ggml_cpy(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // make contiguous
    public static native ggml_tensor ggml_cont(
                ggml_context ctx,
                ggml_tensor a);

    // return view(a), b specifies the new shape
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // return view(a)
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0);

    public static native ggml_tensor ggml_reshape_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1);

    // return view(a)
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape_3d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2);

    public static native ggml_tensor ggml_reshape_4d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3);

    // offset in bytes
    public static native ggml_tensor ggml_view_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_3d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_4d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_permute(
                ggml_context ctx,
                ggml_tensor a,
                int axis0,
                int axis1,
                int axis2,
                int axis3);

    // alias for ggml_permute(ctx, a, 1, 0, 2, 3)
    public static native ggml_tensor ggml_transpose(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_get_rows(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_get_rows_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c);

    public static native ggml_tensor ggml_diag(
            ggml_context ctx,
            ggml_tensor a);

    // set elements above the diagonal to -INF
    public static native ggml_tensor ggml_diag_mask_inf(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_diag_mask_inf_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // set elements above the diagonal to 0
    public static native ggml_tensor ggml_diag_mask_zero(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_diag_mask_zero_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    public static native ggml_tensor ggml_soft_max(
                ggml_context ctx,
                ggml_tensor a);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_soft_max_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_soft_max_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_soft_max_back_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // rotary position embedding
    // if mode & 1 == 1, skip n_past elements
    // if mode & 2 == 1, GPT-NeoX style
    // if mode & 4 == 1, ChatGLM style
    // TODO: avoid creating a new tensor every time
    public static native ggml_tensor ggml_rope(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                int mode,
                int n_ctx);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_rope_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                int mode,
                int n_ctx);

    // rotary position embedding backward, i.e compute dx from dy
    // a - dy
    public static native ggml_tensor ggml_rope_back(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                int mode);

    // alibi position embedding
    // in-place, returns view(a)
    public static native ggml_tensor ggml_alibi(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_head,
                float bias_max);

    // clamp
    // in-place, returns view(a)
    public static native ggml_tensor ggml_clamp(
                ggml_context ctx,
                ggml_tensor a,
                float min,
                float max);

    public static native ggml_tensor ggml_conv_1d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int p0,
                int d0); // dilation

    public static native ggml_tensor ggml_conv_2d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int s1,
                int p0,
                int p1,
                int d0,
                int d1);

    // conv_1d with padding = half
    // alias for ggml_conv_1d(a, b, s, a->ne[0]/2, d)
    public static native ggml_tensor ggml_conv_1d_ph(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s,
                int d);

    public static native ggml_tensor ggml_flash_attn(
                ggml_context ctx,
                ggml_tensor q,
                ggml_tensor k,
                ggml_tensor v,
                @Cast("bool") boolean masked);

    public static native ggml_tensor ggml_flash_attn_back(
               ggml_context ctx,
               ggml_tensor q,
               ggml_tensor k,
               ggml_tensor v,
               ggml_tensor d,
               @Cast("bool") boolean masked);

    public static native ggml_tensor ggml_flash_ff(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b0,
                ggml_tensor b1,
                ggml_tensor c0,
                ggml_tensor c1);

    // partition into non-overlapping windows with padding if needed
    // example:
    // a:   768   64   64    1
    // w:    14
    // res: 768   14   14    25
    // used in sam
    public static native ggml_tensor ggml_win_part(
                ggml_context ctx,
                ggml_tensor a,
                int w);

    // reverse of ggml_win_part
    // used in sam
    public static native ggml_tensor ggml_win_unpart(
                ggml_context ctx,
                ggml_tensor a,
                int w0,
                int h0,
                int w);
// Targeting ../ggml_unary_op_f32_t.java


// Targeting ../ggml_binary_op_f32_t.java


// Targeting ../ggml_custom1_op_f32_t.java


// Targeting ../ggml_custom2_op_f32_t.java


// Targeting ../ggml_custom3_op_f32_t.java



    public static native ggml_tensor ggml_map_unary_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_unary_op_f32_t fun);

    public static native ggml_tensor ggml_map_unary_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_unary_op_f32_t fun);

    public static native ggml_tensor ggml_map_binary_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_binary_op_f32_t fun);

    public static native ggml_tensor ggml_map_binary_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_binary_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom1_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_custom1_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom1_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_custom1_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom2_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_custom2_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom2_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_custom2_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom3_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                       ggml_custom3_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom3_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                       ggml_custom3_op_f32_t fun);

    // loss function

    public static native ggml_tensor ggml_cross_entropy_loss(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_cross_entropy_loss_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c);

    //
    // automatic differentiation
    //

    public static native void ggml_set_param(
                ggml_context ctx,
                ggml_tensor tensor);

    public static native void ggml_build_forward_expand(ggml_cgraph cgraph, ggml_tensor tensor);

    public static native @ByVal ggml_cgraph ggml_build_forward(ggml_tensor tensor);
    public static native @ByVal ggml_cgraph ggml_build_backward(ggml_context ctx, ggml_cgraph gf, @Cast("bool") boolean keep);

    // ggml_graph_plan() has to be called before ggml_graph_compute()
    // when plan.work_size > 0, caller must allocate memory for plan.work_data
    public static native @ByVal ggml_cplan ggml_graph_plan(ggml_cgraph cgraph, int n_threads);
    public static native int ggml_graph_compute(ggml_cgraph cgraph, ggml_cplan cplan);
    public static native void ggml_graph_reset(ggml_cgraph cgraph);

    // same as ggml_graph_compute() but the work data is allocated as a part of the context
    // note: the drawback of this API is that you must have ensured that the context has enough memory for the work data
    public static native void ggml_graph_compute_with_ctx(ggml_context ctx, ggml_cgraph cgraph, int n_threads);

    public static native ggml_tensor ggml_graph_get_tensor(ggml_cgraph cgraph, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_graph_get_tensor(ggml_cgraph cgraph, String name);

    public static native void ggml_graph_export(@Const ggml_cgraph cgraph, @Cast("const char*") BytePointer fname);
    public static native void ggml_graph_export(@Const ggml_cgraph cgraph, String fname);
    public static native @ByVal ggml_cgraph ggml_graph_import(@Cast("const char*") BytePointer fname, @Cast("ggml_context**") PointerPointer ctx_data, @Cast("ggml_context**") PointerPointer ctx_eval);
    public static native @ByVal ggml_cgraph ggml_graph_import(@Cast("const char*") BytePointer fname, @ByPtrPtr ggml_context ctx_data, @ByPtrPtr ggml_context ctx_eval);
    public static native @ByVal ggml_cgraph ggml_graph_import(String fname, @ByPtrPtr ggml_context ctx_data, @ByPtrPtr ggml_context ctx_eval);

    // print info and performance information for the graph
    public static native void ggml_graph_print(@Const ggml_cgraph cgraph);

    // dump the graph into a file using the dot format
    public static native void ggml_graph_dump_dot(@Const ggml_cgraph gb, @Const ggml_cgraph gf, @Cast("const char*") BytePointer filename);
    public static native void ggml_graph_dump_dot(@Const ggml_cgraph gb, @Const ggml_cgraph gf, String filename);

    //
    // optimization
    //

    // optimization methods
    /** enum ggml_opt_type */
    public static final int
        GGML_OPT_ADAM = 0,
        GGML_OPT_LBFGS = 1;

    // linesearch methods
    /** enum ggml_linesearch */
    public static final int
        GGML_LINESEARCH_DEFAULT = 1,

        GGML_LINESEARCH_BACKTRACKING_ARMIJO       = 0,
        GGML_LINESEARCH_BACKTRACKING_WOLFE        = 1,
        GGML_LINESEARCH_BACKTRACKING_STRONG_WOLFE = 2;

    // optimization return values
    /** enum ggml_opt_result */
    public static final int
        GGML_OPT_OK = 0,
        GGML_OPT_DID_NOT_CONVERGE = 1,
        GGML_OPT_NO_CONTEXT = 2,
        GGML_OPT_INVALID_WOLFE = 3,
        GGML_OPT_FAIL = 4,

        GGML_LINESEARCH_FAIL = -128,
        GGML_LINESEARCH_MINIMUM_STEP = -127,
        GGML_LINESEARCH_MAXIMUM_STEP = -126,
        GGML_LINESEARCH_MAXIMUM_ITERATIONS = -125,
        GGML_LINESEARCH_INVALID_PARAMETERS = -124;
// Targeting ../ggml_opt_params.java


// Targeting ../ggml_opt_context.java



    public static native @ByVal ggml_opt_params ggml_opt_default_params(@Cast("ggml_opt_type") int type);

    // optimize the function defined by the tensor f
    public static native @Cast("ggml_opt_result") int ggml_opt(
                ggml_context ctx,
                @ByVal ggml_opt_params params,
                ggml_tensor f);

    // initialize optimizer context
    public static native void ggml_opt_init(
                ggml_context ctx,
                ggml_opt_context opt,
                @ByVal ggml_opt_params params,
                @Cast("int64_t") long nx);

    // continue optimizing the function defined by the tensor f
    public static native @Cast("ggml_opt_result") int ggml_opt_resume(
                ggml_context ctx,
                ggml_opt_context opt,
                ggml_tensor f);

    // continue optimizing the function defined by the tensor f
    public static native @Cast("ggml_opt_result") int ggml_opt_resume_g(
                ggml_context ctx,
                ggml_opt_context opt,
                ggml_tensor f,
                ggml_cgraph gf,
                ggml_cgraph gb);

    //
    // quantization
    //

    public static native @Cast("size_t") long ggml_quantize_q4_0(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q4_0(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q4_0(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
    public static native @Cast("size_t") long ggml_quantize_q4_1(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q4_1(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q4_1(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
    public static native @Cast("size_t") long ggml_quantize_q5_0(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q5_0(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q5_0(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
    public static native @Cast("size_t") long ggml_quantize_q5_1(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q5_1(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q5_1(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
    public static native @Cast("size_t") long ggml_quantize_q8_0(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q8_0(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q8_0(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);

    public static native @Cast("size_t") long ggml_quantize_chunk(@Cast("ggml_type") int type, @Const FloatPointer src, Pointer dst, int start, int n, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_chunk(@Cast("ggml_type") int type, @Const FloatBuffer src, Pointer dst, int start, int n, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_chunk(@Cast("ggml_type") int type, @Const float[] src, Pointer dst, int start, int n, @Cast("int64_t*") long[] hist);

    //
    // system info
    //

    public static native int ggml_cpu_has_avx();
    public static native int ggml_cpu_has_avx2();
    public static native int ggml_cpu_has_avx512();
    public static native int ggml_cpu_has_avx512_vbmi();
    public static native int ggml_cpu_has_avx512_vnni();
    public static native int ggml_cpu_has_fma();
    public static native int ggml_cpu_has_neon();
    public static native int ggml_cpu_has_arm_fma();
    public static native int ggml_cpu_has_f16c();
    public static native int ggml_cpu_has_fp16_va();
    public static native int ggml_cpu_has_wasm_simd();
    public static native int ggml_cpu_has_blas();
    public static native int ggml_cpu_has_cublas();
    public static native int ggml_cpu_has_clblast();
    public static native int ggml_cpu_has_gpublas();
    public static native int ggml_cpu_has_sse3();
    public static native int ggml_cpu_has_vsx();

    //
    // Internal types and functions exposed for tests and benchmarks
    //

// #ifdef  __cplusplus
// restrict not standard in C++
// #define GGML_RESTRICT
// Targeting ../ggml_to_float_t.java


// Targeting ../ggml_from_float_t.java


// Targeting ../ggml_vec_dot_t.java


// Targeting ../ggml_type_traits_t.java



    public static native @ByVal ggml_type_traits_t ggml_internal_get_type_traits(@Cast("ggml_type") int i);

// #ifdef  __cplusplus
// #endif


// Parsed from llama-util.h

// Internal header to be included only by llama.cpp.
// Contains wrappers around OS interfaces.

// #ifndef LLAMA_UTIL_H
// #define LLAMA_UTIL_H

// #include <cstdio>
// #include <cstdint>
// #include <cerrno>
// #include <cstring>
// #include <cstdarg>
// #include <cstdlib>
// #include <climits>

// #include <string>
// #include <vector>
// #include <stdexcept>

// #ifdef __has_include
//     #if __has_include(<unistd.h>)
//         #include <unistd.h>
//         #if defined(_POSIX_MAPPED_FILES)
//             #include <sys/mman.h>
//         #endif
//         #if defined(_POSIX_MEMLOCK_RANGE)
//             #include <sys/resource.h>
//         #endif
//     #endif
// #endif

// #if defined(_WIN32)
// #endif

// #define LLAMA_ASSERT(x)
//     do {
//         if (!(x)) {
//             fprintf(stderr, "LLAMA_ASSERT: %s:%d: %s\n", __FILE__, __LINE__, #x);
//             abort();
//         }
//     } while (0)

// #ifdef __GNUC__
// #ifdef __MINGW32__
public static native @StdString BytePointer format(@Cast("const char*") BytePointer fmt);
public static native @StdString String format(String fmt);
// Targeting ../llama_file.java


// Targeting ../llama_mmap.java


// Targeting ../llama_mlock.java


// Targeting ../llama_buffer.java



// #ifdef GGML_USE_CUBLAS
// #else
// #endif

// #endif


// Parsed from llama.h

// #ifndef LLAMA_H
// #define LLAMA_H

// #include "ggml.h"
// #ifdef GGML_USE_CUBLAS
// #else
public static final int LLAMA_MAX_DEVICES = 1;
// #endif // GGML_USE_CUBLAS
// #include <stddef.h>
// #include <stdint.h>
// #include <stdbool.h>

// #ifdef LLAMA_SHARED
// #else
// #    define LLAMA_API
// #endif

// #ifdef __GNUC__
// #    define DEPRECATED(func, hint) func __attribute__((deprecated(hint)))
// #elif defined(_MSC_VER)
// #    define DEPRECATED(func, hint) __declspec(deprecated(hint)) func
// #else
// #    define DEPRECATED(func, hint) func
// #endif

public static final int LLAMA_FILE_MAGIC_GGJT =        0x67676a74; // 'ggjt'
public static final int LLAMA_FILE_MAGIC_GGLA =        0x67676c61; // 'ggla'
public static final int LLAMA_FILE_MAGIC_GGMF =        0x67676d66; // 'ggmf'
public static final int LLAMA_FILE_MAGIC_GGML =        0x67676d6c; // 'ggml'
public static final int LLAMA_FILE_MAGIC_GGSN =        0x6767736e; // 'ggsn'

public static final int LLAMA_FILE_VERSION =           3;
public static final int LLAMA_FILE_MAGIC =             LLAMA_FILE_MAGIC_GGJT;
public static final int LLAMA_FILE_MAGIC_UNVERSIONED = LLAMA_FILE_MAGIC_GGML;
public static final int LLAMA_SESSION_MAGIC =          LLAMA_FILE_MAGIC_GGSN;
public static final int LLAMA_SESSION_VERSION =        1;

public static final int LLAMA_DEFAULT_SEED =           0xFFFFFFFF;

// #if defined(GGML_USE_CUBLAS) || defined(GGML_USE_CLBLAST) || defined(GGML_USE_METAL)
// Defined when llama.cpp is compiled with support for offloading model layers to GPU.
// #define LLAMA_SUPPORTS_GPU_OFFLOAD
// #endif

// #ifdef __cplusplus
// Targeting ../llama_model.java


// Targeting ../llama_context.java


// Targeting ../llama_token_data.java


// Targeting ../llama_token_data_array.java


// Targeting ../llama_progress_callback.java


// Targeting ../llama_context_params.java


    // model file types
    /** enum llama_ftype */
    public static final int
        LLAMA_FTYPE_ALL_F32              = 0,
        LLAMA_FTYPE_MOSTLY_F16           = 1, // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_0          = 2, // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_1          = 3, // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4, // tok_embeddings.weight and output.weight are F16
        // LLAMA_FTYPE_MOSTLY_Q4_2       = 5, // support has been removed
        // LLAMA_FTYPE_MOSTLY_Q4_3       = 6, // support has been removed
        LLAMA_FTYPE_MOSTLY_Q8_0          = 7, // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q5_0          = 8, // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q5_1          = 9, // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q2_K          = 10,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q3_K_S        = 11,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q3_K_M        = 12,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q3_K_L        = 13,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_K_S        = 14,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_K_M        = 15,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q5_K_S        = 16,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q5_K_M        = 17,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q6_K          = 18;// except 1d tensors
// Targeting ../llama_model_quantize_params.java


// Targeting ../llama_timings.java



    public static native @ByVal llama_context_params llama_context_default_params();
    public static native @ByVal llama_model_quantize_params llama_model_quantize_default_params();

    public static native @Cast("bool") boolean llama_mmap_supported();
    public static native @Cast("bool") boolean llama_mlock_supported();

    // TODO: not great API - very likely to change
    // Initialize the llama + ggml backend
    // If numa is true, use NUMA optimizations
    // Call once at the start of the program
    public static native void llama_backend_init(@Cast("bool") boolean numa);
    // Call once at the end of the program - currently only used for MPI
    public static native void llama_backend_free();

    public static native @Cast("int64_t") long llama_time_us();

    public static native llama_model llama_load_model_from_file(
                                 @Cast("const char*") BytePointer path_model,
                @ByVal llama_context_params params);
    public static native llama_model llama_load_model_from_file(
                                 String path_model,
                @ByVal llama_context_params params);

    public static native void llama_free_model(llama_model model);

    public static native llama_context llama_new_context_with_model(
                         llama_model model,
                @ByVal llama_context_params params);

    // Various functions for loading a ggml llama model.
    // Allocate (almost) all memory needed for the model.
    // Return NULL on failure
    

    // Frees all allocated memory
    public static native void llama_free(llama_context ctx);

    // Returns 0 on success
    public static native int llama_model_quantize(
                @Cast("const char*") BytePointer fname_inp,
                @Cast("const char*") BytePointer fname_out,
                @Const llama_model_quantize_params params);
    public static native int llama_model_quantize(
                String fname_inp,
                String fname_out,
                @Const llama_model_quantize_params params);

    // Apply a LoRA adapter to a loaded model
    // path_base_model is the path to a higher quality model to use as a base for
    // the layers modified by the adapter. Can be NULL to use the current loaded model.
    // The model needs to be reloaded before applying a new adapter, otherwise the adapter
    // will be applied on top of the previous one
    // Returns 0 on success
    

    public static native int llama_model_apply_lora_from_file(
                @Const llama_model model,
                          @Cast("const char*") BytePointer path_lora,
                          @Cast("const char*") BytePointer path_base_model,
                                 int n_threads);
    public static native int llama_model_apply_lora_from_file(
                @Const llama_model model,
                          String path_lora,
                          String path_base_model,
                                 int n_threads);

    // Returns the number of tokens in the KV cache
    public static native int llama_get_kv_cache_token_count(@Const llama_context ctx);

    // Sets the current rng seed.
    public static native void llama_set_rng_seed(llama_context ctx, @Cast("uint32_t") int seed);

    // Returns the maximum size in bytes of the state (rng, logits, embedding
    // and kv_cache) - will often be smaller after compacting tokens
    public static native @Cast("size_t") long llama_get_state_size(@Const llama_context ctx);

    // Copies the state to the specified destination address.
    // Destination needs to have allocated enough memory.
    // Returns the number of bytes copied
    public static native @Cast("size_t") long llama_copy_state_data(llama_context ctx, @Cast("uint8_t*") BytePointer dst);
    public static native @Cast("size_t") long llama_copy_state_data(llama_context ctx, @Cast("uint8_t*") ByteBuffer dst);
    public static native @Cast("size_t") long llama_copy_state_data(llama_context ctx, @Cast("uint8_t*") byte[] dst);

    // Set the state reading from the specified address
    // Returns the number of bytes read
    public static native @Cast("size_t") long llama_set_state_data(llama_context ctx, @Cast("uint8_t*") BytePointer src);
    public static native @Cast("size_t") long llama_set_state_data(llama_context ctx, @Cast("uint8_t*") ByteBuffer src);
    public static native @Cast("size_t") long llama_set_state_data(llama_context ctx, @Cast("uint8_t*") byte[] src);

    // Save/load session file
    public static native @Cast("bool") boolean llama_load_session_file(llama_context ctx, @Cast("const char*") BytePointer path_session, @Cast("llama_token*") IntPointer tokens_out, @Cast("size_t") long n_token_capacity, @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(llama_context ctx, String path_session, @Cast("llama_token*") IntBuffer tokens_out, @Cast("size_t") long n_token_capacity, @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(llama_context ctx, @Cast("const char*") BytePointer path_session, @Cast("llama_token*") int[] tokens_out, @Cast("size_t") long n_token_capacity, @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(llama_context ctx, String path_session, @Cast("llama_token*") IntPointer tokens_out, @Cast("size_t") long n_token_capacity, @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(llama_context ctx, @Cast("const char*") BytePointer path_session, @Cast("llama_token*") IntBuffer tokens_out, @Cast("size_t") long n_token_capacity, @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(llama_context ctx, String path_session, @Cast("llama_token*") int[] tokens_out, @Cast("size_t") long n_token_capacity, @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_save_session_file(llama_context ctx, @Cast("const char*") BytePointer path_session, @Cast("const llama_token*") IntPointer tokens, @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(llama_context ctx, String path_session, @Cast("const llama_token*") IntBuffer tokens, @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(llama_context ctx, @Cast("const char*") BytePointer path_session, @Cast("const llama_token*") int[] tokens, @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(llama_context ctx, String path_session, @Cast("const llama_token*") IntPointer tokens, @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(llama_context ctx, @Cast("const char*") BytePointer path_session, @Cast("const llama_token*") IntBuffer tokens, @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(llama_context ctx, String path_session, @Cast("const llama_token*") int[] tokens, @Cast("size_t") long n_token_count);

    // Run the llama inference to obtain the logits and probabilities for the next token.
    // tokens + n_tokens is the provided batch of new tokens to process
    // n_past is the number of tokens to use from previous eval calls
    // Returns 0 on success
    public static native int llama_eval(
                llama_context ctx,
                   @Cast("const llama_token*") IntPointer tokens,
                                 int n_tokens,
                                 int n_past,
                                 int n_threads);
    public static native int llama_eval(
                llama_context ctx,
                   @Cast("const llama_token*") IntBuffer tokens,
                                 int n_tokens,
                                 int n_past,
                                 int n_threads);
    public static native int llama_eval(
                llama_context ctx,
                   @Cast("const llama_token*") int[] tokens,
                                 int n_tokens,
                                 int n_past,
                                 int n_threads);

    // Same as llama_eval, but use float matrix input directly.
    public static native int llama_eval_embd(
                llama_context ctx,
                         @Const FloatPointer embd,
                                 int n_tokens,
                                 int n_past,
                                 int n_threads);
    public static native int llama_eval_embd(
                llama_context ctx,
                         @Const FloatBuffer embd,
                                 int n_tokens,
                                 int n_past,
                                 int n_threads);
    public static native int llama_eval_embd(
                llama_context ctx,
                         @Const float[] embd,
                                 int n_tokens,
                                 int n_past,
                                 int n_threads);

    // Export a static computation graph for context of 511 and batch size of 1
    // NOTE: since this functionality is mostly for debugging and demonstration purposes, we hardcode these
    //       parameters here to keep things simple
    // IMPORTANT: do not use for anything else other than debugging and testing!
    public static native int llama_eval_export(llama_context ctx, @Cast("const char*") BytePointer fname);
    public static native int llama_eval_export(llama_context ctx, String fname);

    // Convert the provided text into tokens.
    // The tokens pointer must be large enough to hold the resulting tokens.
    // Returns the number of tokens on success, no more than n_max_tokens
    // Returns a negative number on failure - the number of tokens that would have been returned
    // TODO: not sure if correct
    public static native int llama_tokenize(
                llama_context ctx,
                          @Cast("const char*") BytePointer text,
                         @Cast("llama_token*") IntPointer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize(
                llama_context ctx,
                          String text,
                         @Cast("llama_token*") IntBuffer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize(
                llama_context ctx,
                          @Cast("const char*") BytePointer text,
                         @Cast("llama_token*") int[] tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize(
                llama_context ctx,
                          String text,
                         @Cast("llama_token*") IntPointer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize(
                llama_context ctx,
                          @Cast("const char*") BytePointer text,
                         @Cast("llama_token*") IntBuffer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize(
                llama_context ctx,
                          String text,
                         @Cast("llama_token*") int[] tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);

    public static native int llama_n_vocab(@Const llama_context ctx);
    public static native int llama_n_ctx(@Const llama_context ctx);
    public static native int llama_n_embd(@Const llama_context ctx);

    // Get the vocabulary as output parameters.
    // Returns number of results.
    public static native int llama_get_vocab(
                @Const llama_context ctx,
                              @Cast("const char**") PointerPointer strings,
                                     FloatPointer scores,
                                       int _capacity);
    public static native int llama_get_vocab(
                @Const llama_context ctx,
                              @Cast("const char**") @ByPtrPtr BytePointer strings,
                                     FloatPointer scores,
                                       int _capacity);
    public static native int llama_get_vocab(
                @Const llama_context ctx,
                              @Cast("const char**") @ByPtrPtr ByteBuffer strings,
                                     FloatBuffer scores,
                                       int _capacity);
    public static native int llama_get_vocab(
                @Const llama_context ctx,
                              @Cast("const char**") @ByPtrPtr byte[] strings,
                                     float[] scores,
                                       int _capacity);

    // Token logits obtained from the last call to llama_eval()
    // The logits for the last token are stored in the last row
    // Can be mutated in order to change the probabilities of the next token
    // Rows: n_tokens
    // Cols: n_vocab
    public static native FloatPointer llama_get_logits(llama_context ctx);

    // Get the embeddings for the input
    // shape: [n_embd] (1-dimensional)
    public static native FloatPointer llama_get_embeddings(llama_context ctx);

    // Token Id -> String. Uses the vocabulary in the provided context
    public static native @Cast("const char*") BytePointer llama_token_to_str(@Const llama_context ctx, @Cast("llama_token") int token);

    // Special tokens
    public static native @Cast("llama_token") int llama_token_bos();  // beginning-of-sentence
    public static native @Cast("llama_token") int llama_token_eos();  // end-of-sentence
    public static native @Cast("llama_token") int llama_token_nl();   // next-line

    // Sampling functions

    /** \details Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix. */
    public static native void llama_sample_repetition_penalty(llama_context ctx, llama_token_data_array candidates, @Cast("const llama_token*") IntPointer last_tokens, @Cast("size_t") long last_tokens_size, float penalty);
    public static native void llama_sample_repetition_penalty(llama_context ctx, llama_token_data_array candidates, @Cast("const llama_token*") IntBuffer last_tokens, @Cast("size_t") long last_tokens_size, float penalty);
    public static native void llama_sample_repetition_penalty(llama_context ctx, llama_token_data_array candidates, @Cast("const llama_token*") int[] last_tokens, @Cast("size_t") long last_tokens_size, float penalty);

    /** \details Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details. */
    public static native void llama_sample_frequency_and_presence_penalties(llama_context ctx, llama_token_data_array candidates, @Cast("const llama_token*") IntPointer last_tokens, @Cast("size_t") long last_tokens_size, float alpha_frequency, float alpha_presence);
    public static native void llama_sample_frequency_and_presence_penalties(llama_context ctx, llama_token_data_array candidates, @Cast("const llama_token*") IntBuffer last_tokens, @Cast("size_t") long last_tokens_size, float alpha_frequency, float alpha_presence);
    public static native void llama_sample_frequency_and_presence_penalties(llama_context ctx, llama_token_data_array candidates, @Cast("const llama_token*") int[] last_tokens, @Cast("size_t") long last_tokens_size, float alpha_frequency, float alpha_presence);

    /** \details Apply classifier-free guidance to the logits as described in academic paper "Stay on topic with Classifier-Free Guidance" https://arxiv.org/abs/2306.17806
     *  @param candidates A vector of {@code llama_token_data} containing the candidate tokens, the logits must be directly extracted from the original generation context without being sorted.
     *  \params guidance_ctx A separate context from the same model. Other than a negative prompt at the beginning, it should have all generated and user input tokens copied from the main context.
     *  \params scale Guidance strength. 1.0f means no guidance. Higher values mean stronger guidance.
     *  \params smooth_factor Smooth factor between guidance logits and original logits. 1.0f means only use guidance logits. 0.0f means only original logits. */
    public static native void llama_sample_classifier_free_guidance(
                  llama_context ctx,
                llama_token_data_array candidates,
                  llama_context guidance_ctx,
                                 float scale,
                                 float smooth_factor);

    /** \details Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits. */
    public static native void llama_sample_softmax(llama_context ctx, llama_token_data_array candidates);

    /** \details Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751 */
    public static native void llama_sample_top_k(llama_context ctx, llama_token_data_array candidates, int k, @Cast("size_t") long min_keep);

    /** \details Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751 */
    public static native void llama_sample_top_p(llama_context ctx, llama_token_data_array candidates, float p, @Cast("size_t") long min_keep);

    /** \details Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/. */
    public static native void llama_sample_tail_free(llama_context ctx, llama_token_data_array candidates, float z, @Cast("size_t") long min_keep);

    /** \details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666. */
    public static native void llama_sample_typical(llama_context ctx, llama_token_data_array candidates, float p, @Cast("size_t") long min_keep);
    public static native void llama_sample_temperature(llama_context ctx, llama_token_data_array candidates, float temp);

    /** \details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
     *  @param candidates A vector of {@code llama_token_data} containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
     *  @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
     *  @param eta The learning rate used to update {@code mu} based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause {@code mu} to be updated more quickly, while a smaller learning rate will result in slower updates.
     *  @param m The number of tokens considered in the estimation of {@code s_hat}. This is an arbitrary value that is used to calculate {@code s_hat}, which in turn helps to calculate the value of {@code k}. In the paper, they use {@code m = 100}, but you can experiment with different values to see how it affects the performance of the algorithm.
     *  @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy ({@code 2 * tau}) and is updated in the algorithm based on the error between the target and observed surprisal. */
    public static native @Cast("llama_token") int llama_sample_token_mirostat(llama_context ctx, llama_token_data_array candidates, float tau, float eta, int m, FloatPointer mu);
    public static native @Cast("llama_token") int llama_sample_token_mirostat(llama_context ctx, llama_token_data_array candidates, float tau, float eta, int m, FloatBuffer mu);
    public static native @Cast("llama_token") int llama_sample_token_mirostat(llama_context ctx, llama_token_data_array candidates, float tau, float eta, int m, float[] mu);

    /** \details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
     *  @param candidates A vector of {@code llama_token_data} containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
     *  @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
     *  @param eta The learning rate used to update {@code mu} based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause {@code mu} to be updated more quickly, while a smaller learning rate will result in slower updates.
     *  @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy ({@code 2 * tau}) and is updated in the algorithm based on the error between the target and observed surprisal. */
    public static native @Cast("llama_token") int llama_sample_token_mirostat_v2(llama_context ctx, llama_token_data_array candidates, float tau, float eta, FloatPointer mu);
    public static native @Cast("llama_token") int llama_sample_token_mirostat_v2(llama_context ctx, llama_token_data_array candidates, float tau, float eta, FloatBuffer mu);
    public static native @Cast("llama_token") int llama_sample_token_mirostat_v2(llama_context ctx, llama_token_data_array candidates, float tau, float eta, float[] mu);

    /** \details Selects the token with the highest probability. */
    public static native @Cast("llama_token") int llama_sample_token_greedy(llama_context ctx, llama_token_data_array candidates);

    /** \details Randomly selects a token from the candidates based on their probabilities. */
    public static native @Cast("llama_token") int llama_sample_token(llama_context ctx, llama_token_data_array candidates);

    // Performance information
    public static native @ByVal llama_timings llama_get_timings(llama_context ctx);
    public static native void llama_print_timings(llama_context ctx);
    public static native void llama_reset_timings(llama_context ctx);

    // Print system information
    public static native @Cast("const char*") BytePointer llama_print_system_info();

// #ifdef __cplusplus
// #endif

// Internal API to be implemented by llama.cpp and used by tests/benchmarks only
// #ifdef LLAMA_API_INTERNAL

// #endif

// #endif // LLAMA_H


}
