// Targeted by JavaCPP version 1.5.10-SNAPSHOT: DO NOT EDIT THIS FILE

package org.javallm.llamacpp.global;

import org.javallm.llamacpp.*;

import java.nio.*;
import org.bytedeco.javacpp.*;
import org.bytedeco.javacpp.annotation.*;

public class llama extends org.javallm.llamacpp.presets.llama {
    static { Loader.load(); }

// Parsed from ggml.h

// #pragma once

//
// GGML Tensor Library
//
// This documentation is still a work in progress.
// If you wish some specific topics to be covered, feel free to drop a comment:
//
//   https://github.com/ggerganov/whisper.cpp/issues/40
//
// ## Overview
//
// This library implements:
//
//  - a set of tensor operations
//  - automatic differentiation
//  - basic optimization algorithms
//
// The aim of this library is to provide a minimalistic approach for various machine learning tasks. This includes,
// but is not limited to, the following:
//
//  - linear regression
//  - support vector machines
//  - neural networks
//
// The library allows the user to define a certain function using the available tensor operations. This function
// definition is represented internally via a computation graph. Each tensor operation in the function definition
// corresponds to a node in the graph. Having the computation graph defined, the user can choose to compute the
// function's value and/or its gradient with respect to the input variables. Optionally, the function can be optimized
// using one of the available optimization algorithms.
//
// For example, here we define the function: f(x) = a*x^2 + b
//
//   {
//       struct ggml_init_params params = {
//           .mem_size   = 16*1024*1024,
//           .mem_buffer = NULL,
//       };
//
//       // memory allocation happens here
//       struct ggml_context * ctx = ggml_init(params);
//
//       struct ggml_tensor * x = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//
//       ggml_set_param(ctx, x); // x is an input variable
//
//       struct ggml_tensor * a  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//       struct ggml_tensor * b  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);
//       struct ggml_tensor * x2 = ggml_mul(ctx, x, x);
//       struct ggml_tensor * f  = ggml_add(ctx, ggml_mul(ctx, a, x2), b);
//
//       ...
//   }
//
// Notice that the function definition above does not involve any actual computation. The computation is performed only
// when the user explicitly requests it. For example, to compute the function's value at x = 2.0:
//
//   {
//       ...
//
//       struct ggml_cgraph gf = ggml_build_forward(f);
//
//       // set the input variable and parameter values
//       ggml_set_f32(x, 2.0f);
//       ggml_set_f32(a, 3.0f);
//       ggml_set_f32(b, 4.0f);
//
//       ggml_graph_compute_with_ctx(ctx, &gf, n_threads);
//
//       printf("f = %f\n", ggml_get_f32_1d(f, 0));
//
//       ...
//   }
//
// The actual computation is performed in the ggml_graph_compute() function.
//
// The ggml_new_tensor_...() functions create new tensors. They are allocated in the memory buffer provided to the
// ggml_init() function. You have to be careful not to exceed the memory buffer size. Therefore, you have to know
// in advance how much memory you need for your computation. Alternatively, you can allocate a large enough memory
// and after defining the computation graph, call the ggml_used_mem() function to find out how much memory was
// actually needed.
//
// The ggml_set_param() function marks a tensor as an input variable. This is used by the automatic
// differentiation and optimization algorithms.
//
// The described approach allows to define the function graph once and then compute its forward or backward graphs
// multiple times. All computations will use the same memory buffer allocated in the ggml_init() function. This way
// the user can avoid the memory allocation overhead at runtime.
//
// The library supports multi-dimensional tensors - up to 4 dimensions. The FP16 and FP32 data types are first class
// citizens, but in theory the library can be extended to support FP8 and integer data types.
//
// Each tensor operation produces a new tensor. Initially the library was envisioned to support only the use of unary
// and binary operations. Most of the available operations fall into one of these two categories. With time, it became
// clear that the library needs to support more complex operations. The way to support these operations is not clear
// yet, but a few examples are demonstrated in the following operations:
//
//   - ggml_permute()
//   - ggml_conv_1d_1s()
//   - ggml_conv_1d_2s()
//
// For each tensor operator, the library implements a forward and backward computation function. The forward function
// computes the output tensor value given the input tensor values. The backward function computes the adjoint of the
// input tensors given the adjoint of the output tensor. For a detailed explanation of what this means, take a
// calculus class, or watch the following video:
//
//   What is Automatic Differentiation?
//   https://www.youtube.com/watch?v=wG_nF1awSSY
//
//
// ## Tensor data (struct ggml_tensor)
//
// The tensors are stored in memory via the ggml_tensor struct. The structure provides information about the size of
// the tensor, the data type, and the memory buffer where the tensor data is stored. Additionally, it contains
// pointers to the "source" tensors - i.e. the tensors that were used to compute the current tensor. For example:
//
//   {
//       struct ggml_tensor * c = ggml_add(ctx, a, b);
//
//       assert(c->src[0] == a);
//       assert(c->src[1] == b);
//   }
//
// The multi-dimensional tensors are stored in row-major order. The ggml_tensor struct contains fields for the
// number of elements in each dimension ("ne") as well as the number of bytes ("nb", a.k.a. stride). This allows
// to store tensors that are not contiguous in memory, which is useful for operations such as transposition and
// permutation. All tensor operations have to take the stride into account and not assume that the tensor is
// contiguous in memory.
//
// The data of the tensor is accessed via the "data" pointer. For example:
//
//   {
//       struct ggml_tensor * a = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, 2, 3);
//
//       // a[2, 1] = 1.0f;
//       *(float *) ((char *) a->data + 2*a->nb[1] + 1*a->nb[0]) = 1.0f;
//
//       // a[0, 2] = 2.0f;
//       *(float *) ((char *) a->data + 0*a->nb[1] + 2*a->nb[0]) = 2.0f;
//
//       ...
//   }
//
// Alternatively, there are helper functions, such as ggml_get_f32_1d() and ggml_set_f32_1d() that can be used.
//
// ## The matrix multiplication operator (ggml_mul_mat)
//
// TODO
//
//
// ## Multi-threading
//
// TODO
//
//
// ## Overview of ggml.c
//
// TODO
//
//
// ## SIMD optimizations
//
// TODO
//
//
// ## Debugging ggml
//
// TODO
//
//

// #ifdef GGML_SHARED
// #else
// #    define GGML_API
// #endif

// TODO: support for clang
// #ifdef __GNUC__
// #    define GGML_DEPRECATED(func, hint) func __attribute__((deprecated(hint)))
// #elif defined(_MSC_VER)
// #    define GGML_DEPRECATED(func, hint) __declspec(deprecated(hint)) func
// #else
// #    define GGML_DEPRECATED(func, hint) func
// #endif

// #include <stdint.h>
// #include <stddef.h>
// #include <stdbool.h>

public static final int GGML_FILE_MAGIC =   0x67676d6c; // "ggml"
public static final int GGML_FILE_VERSION = 1;

public static final int GGML_QNT_VERSION =        2;    // bump this on quantization format changes
public static final int GGML_QNT_VERSION_FACTOR = 1000; // do not change this

public static final int GGML_MAX_DIMS =          4;
public static final int GGML_MAX_NODES =         4096;
public static final int GGML_MAX_PARAMS =        256;
public static final int GGML_MAX_CONTEXTS =      64;
public static final int GGML_MAX_SRC =           6;
public static final int GGML_MAX_NAME =          64;
public static final int GGML_MAX_OP_PARAMS =     32;
public static final int GGML_DEFAULT_N_THREADS = 4;


public static final int GGML_EXIT_SUCCESS = 0;
public static final int GGML_EXIT_ABORTED = 1;

public static final int GGUF_MAGIC =   0x46554747; // "GGUF"
public static final int GGUF_VERSION = 1;

public static final int GGUF_DEFAULT_ALIGNMENT = 32;

// #define GGML_UNUSED(x) (void)(x)

// #define GGML_PAD(x, n) (((x) + (n) - 1) & ~((n) - 1))

// #define GGML_ASSERT(x)
//     do {
//         if (!(x)) {
//             fprintf(stderr, "GGML_ASSERT: %s:%d: %s\n", __FILE__, __LINE__, #x);
//             abort();
//         }
//     } while (0)

// used to copy the number of elements and stride in bytes of tensors into local variables.
// main purpose is to reduce code duplication and improve readability.
//
// example:
//
//    GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne);
//    GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb);
//
// #define GGML_TENSOR_LOCALS_1(type, prefix, pointer, array)
//     const type prefix##0 = (pointer)->array[0];
//     GGML_UNUSED(prefix##0);
// #define GGML_TENSOR_LOCALS_2(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_1    (type, prefix, pointer, array)
//     const type prefix##1 = (pointer)->array[1];
//     GGML_UNUSED(prefix##1);
// #define GGML_TENSOR_LOCALS_3(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_2    (type, prefix, pointer, array)
//     const type prefix##2 = (pointer)->array[2];
//     GGML_UNUSED(prefix##2);
// #define GGML_TENSOR_LOCALS(type, prefix, pointer, array)
//     GGML_TENSOR_LOCALS_3  (type, prefix, pointer, array)
//     const type prefix##3 = (pointer)->array[3];
//     GGML_UNUSED(prefix##3);

// #ifdef  __cplusplus
// #endif

// #if defined(__ARM_NEON) && defined(__CUDACC__)
// #elif defined(__ARM_NEON)
// #else
// #endif

    // convert FP16 <-> FP32
    public static native float ggml_fp16_to_fp32(@Cast("ggml_fp16_t") short x);
    public static native @Cast("ggml_fp16_t") short ggml_fp32_to_fp16(float x);

    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") ShortPointer x, FloatPointer y, int n);
    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") ShortBuffer x, FloatBuffer y, int n);
    public static native void ggml_fp16_to_fp32_row(@Cast("const ggml_fp16_t*") short[] x, float[] y, int n);
    public static native void ggml_fp32_to_fp16_row(@Const FloatPointer x, @Cast("ggml_fp16_t*") ShortPointer y, int n);
    public static native void ggml_fp32_to_fp16_row(@Const FloatBuffer x, @Cast("ggml_fp16_t*") ShortBuffer y, int n);
    public static native void ggml_fp32_to_fp16_row(@Const float[] x, @Cast("ggml_fp16_t*") short[] y, int n);
// Targeting ../ggml_context.java



    /** enum ggml_type */
    public static final int
        GGML_TYPE_F32  = 0,
        GGML_TYPE_F16  = 1,
        GGML_TYPE_Q4_0 = 2,
        GGML_TYPE_Q4_1 = 3,
        // GGML_TYPE_Q4_2 = 4, support has been removed
        // GGML_TYPE_Q4_3 (5) support has been removed
        GGML_TYPE_Q5_0 = 6,
        GGML_TYPE_Q5_1 = 7,
        GGML_TYPE_Q8_0 = 8,
        GGML_TYPE_Q8_1 = 9,
        // k-quantizations
        GGML_TYPE_Q2_K = 10,
        GGML_TYPE_Q3_K = 11,
        GGML_TYPE_Q4_K = 12,
        GGML_TYPE_Q5_K = 13,
        GGML_TYPE_Q6_K = 14,
        GGML_TYPE_Q8_K = 15,
        GGML_TYPE_I8 = 16,
        GGML_TYPE_I16 = 17,
        GGML_TYPE_I32 = 18,
        GGML_TYPE_COUNT = 19;

    /** enum ggml_backend */
    public static final int
        GGML_BACKEND_CPU = 0,
        GGML_BACKEND_GPU = 10,
        GGML_BACKEND_GPU_SPLIT = 20;

    // model file types
    /** enum ggml_ftype */
    public static final int
        GGML_FTYPE_UNKNOWN     = -1,
        GGML_FTYPE_ALL_F32     = 0,
        GGML_FTYPE_MOSTLY_F16  = 1,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_0 = 2,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_1 = 3,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4, // tok_embeddings.weight and output.weight are F16
        GGML_FTYPE_MOSTLY_Q8_0 = 7,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_0 = 8,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_1 = 9,  // except 1d tensors
        GGML_FTYPE_MOSTLY_Q2_K = 10, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q3_K = 11, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q4_K = 12, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q5_K = 13, // except 1d tensors
        GGML_FTYPE_MOSTLY_Q6_K = 14; // except 1d tensors

    // available tensor operations:
    /** enum ggml_op */
    public static final int
        GGML_OP_NONE = 0,

        GGML_OP_DUP = 1,
        GGML_OP_ADD = 2,
        GGML_OP_ADD1 = 3,
        GGML_OP_ACC = 4,
        GGML_OP_SUB = 5,
        GGML_OP_MUL = 6,
        GGML_OP_DIV = 7,
        GGML_OP_SQR = 8,
        GGML_OP_SQRT = 9,
        GGML_OP_LOG = 10,
        GGML_OP_SUM = 11,
        GGML_OP_SUM_ROWS = 12,
        GGML_OP_MEAN = 13,
        GGML_OP_ARGMAX = 14,
        GGML_OP_REPEAT = 15,
        GGML_OP_REPEAT_BACK = 16,
        GGML_OP_CONCAT = 17,
        GGML_OP_SILU_BACK = 18,
        GGML_OP_NORM = 19, // normalize
        GGML_OP_RMS_NORM = 20,
        GGML_OP_RMS_NORM_BACK = 21,
        GGML_OP_GROUP_NORM = 22,

        GGML_OP_MUL_MAT = 23,
        GGML_OP_OUT_PROD = 24,

        GGML_OP_SCALE = 25,
        GGML_OP_SET = 26,
        GGML_OP_CPY = 27,
        GGML_OP_CONT = 28,
        GGML_OP_RESHAPE = 29,
        GGML_OP_VIEW = 30,
        GGML_OP_PERMUTE = 31,
        GGML_OP_TRANSPOSE = 32,
        GGML_OP_GET_ROWS = 33,
        GGML_OP_GET_ROWS_BACK = 34,
        GGML_OP_DIAG = 35,
        GGML_OP_DIAG_MASK_INF = 36,
        GGML_OP_DIAG_MASK_ZERO = 37,
        GGML_OP_SOFT_MAX = 38,
        GGML_OP_SOFT_MAX_BACK = 39,
        GGML_OP_ROPE = 40,
        GGML_OP_ROPE_BACK = 41,
        GGML_OP_ALIBI = 42,
        GGML_OP_CLAMP = 43,
        GGML_OP_CONV_1D = 44,
        GGML_OP_CONV_2D = 45,
        GGML_OP_CONV_TRANSPOSE_2D = 46,
        GGML_OP_POOL_1D = 47,
        GGML_OP_POOL_2D = 48,

        GGML_OP_UPSCALE = 49, // nearest interpolate

        GGML_OP_FLASH_ATTN = 50,
        GGML_OP_FLASH_FF = 51,
        GGML_OP_FLASH_ATTN_BACK = 52,
        GGML_OP_WIN_PART = 53,
        GGML_OP_WIN_UNPART = 54,
        GGML_OP_GET_REL_POS = 55,
        GGML_OP_ADD_REL_POS = 56,

        GGML_OP_UNARY = 57,

        GGML_OP_MAP_UNARY = 58,
        GGML_OP_MAP_BINARY = 59,

        GGML_OP_MAP_CUSTOM1_F32 = 60,
        GGML_OP_MAP_CUSTOM2_F32 = 61,
        GGML_OP_MAP_CUSTOM3_F32 = 62,

        GGML_OP_MAP_CUSTOM1 = 63,
        GGML_OP_MAP_CUSTOM2 = 64,
        GGML_OP_MAP_CUSTOM3 = 65,

        GGML_OP_CROSS_ENTROPY_LOSS = 66,
        GGML_OP_CROSS_ENTROPY_LOSS_BACK = 67,

        GGML_OP_COUNT = 68;

    /** enum ggml_unary_op */
    public static final int
        GGML_UNARY_OP_ABS = 0,
        GGML_UNARY_OP_SGN = 1,
        GGML_UNARY_OP_NEG = 2,
        GGML_UNARY_OP_STEP = 3,
        GGML_UNARY_OP_TANH = 4,
        GGML_UNARY_OP_ELU = 5,
        GGML_UNARY_OP_RELU = 6,
        GGML_UNARY_OP_GELU = 7,
        GGML_UNARY_OP_GELU_QUICK = 8,
        GGML_UNARY_OP_SILU = 9;

    /** enum ggml_object_type */
    public static final int
        GGML_OBJECT_TENSOR = 0,
        GGML_OBJECT_GRAPH = 1,
        GGML_OBJECT_WORK_BUFFER = 2;
// Targeting ../ggml_object.java



    @MemberGetter public static native @Cast("const size_t") long GGML_OBJECT_SIZE();
    public static final long GGML_OBJECT_SIZE = GGML_OBJECT_SIZE();
// Targeting ../ggml_tensor.java



    @MemberGetter public static native @Cast("const size_t") long GGML_TENSOR_SIZE();
    public static final long GGML_TENSOR_SIZE = GGML_TENSOR_SIZE();
// Targeting ../ggml_cplan.java



    // next prime after GGML_MAX_NODES
    // #define GGML_GRAPH_HASHTABLE_SIZE 4099
    // next prime after GGML_MAX_NODES * 2 (nodes + leafs)
    public static final int GGML_GRAPH_HASHTABLE_SIZE = 8273;
// Targeting ../ggml_cgraph.java



    @MemberGetter public static native @Cast("const size_t") long GGML_GRAPH_SIZE();
    public static final long GGML_GRAPH_SIZE = GGML_GRAPH_SIZE();
// Targeting ../ggml_scratch.java


// Targeting ../ggml_init_params.java




    // compute types

    // NOTE: the INIT or FINALIZE pass is not scheduled unless explicitly enabled.
    // This behavior was changed since https://github.com/ggerganov/llama.cpp/pull/1995.
    /** enum ggml_task_type */
    public static final int
        GGML_TASK_INIT = 0,
        GGML_TASK_COMPUTE = 1,
        GGML_TASK_FINALIZE = 2;
// Targeting ../ggml_compute_params.java



    // misc

    public static native void ggml_time_init(); // call this once at the beginning of the program
    public static native @Cast("int64_t") long ggml_time_ms();
    public static native @Cast("int64_t") long ggml_time_us();
    public static native @Cast("int64_t") long ggml_cycles();
    public static native @Cast("int64_t") long ggml_cycles_per_ms();

    public static native void ggml_numa_init(); // call once for better performance on NUMA systems
    public static native @Cast("bool") boolean ggml_is_numa(); // true if init detected that system has >1 NUMA node

    public static native void ggml_print_object(@Const ggml_object obj);
    public static native void ggml_print_objects(@Const ggml_context ctx);

    public static native @Cast("int64_t") long ggml_nelements(@Const ggml_tensor tensor);
    public static native @Cast("int64_t") long ggml_nrows(@Const ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_nbytes(@Const ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_nbytes_pad(@Const ggml_tensor tensor); // same as ggml_nbytes() but padded to GGML_MEM_ALIGN
    public static native @Cast("size_t") long ggml_nbytes_split(@Const ggml_tensor tensor, int nrows_split);

    public static native int ggml_blck_size(@Cast("ggml_type") int type);
    public static native @Cast("size_t") long ggml_type_size(@Cast("ggml_type") int type); // size in bytes for all elements in a block
    public static native float ggml_type_sizef(@Cast("ggml_type") int type); // ggml_type_size()/ggml_blck_size() as float

    public static native @Cast("const char*") BytePointer ggml_type_name(@Cast("ggml_type") int type);
    public static native @Cast("const char*") BytePointer ggml_op_name(@Cast("ggml_op") int op);
    public static native @Cast("const char*") BytePointer ggml_op_symbol(@Cast("ggml_op") int op);

    public static native @Cast("size_t") long ggml_element_size(@Const ggml_tensor tensor);

    public static native @Cast("bool") boolean ggml_is_quantized(@Cast("ggml_type") int type);

    // TODO: temporary until model loading of ggml examples is refactored
    public static native @Cast("ggml_type") int ggml_ftype_to_ggml_type(@Cast("ggml_ftype") int ftype);

    public static native @Cast("bool") boolean ggml_is_transposed(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_contiguous(@Const ggml_tensor tensor);
    public static native @Cast("bool") boolean ggml_is_permuted(@Const ggml_tensor tensor);

    public static native @Cast("bool") boolean ggml_are_same_shape(@Const ggml_tensor t0, @Const ggml_tensor t1);

    // use this to compute the memory overhead of a tensor
    public static native @Cast("size_t") long ggml_tensor_overhead();

    // main

    public static native ggml_context ggml_init(@ByVal ggml_init_params params);
    public static native void ggml_free(ggml_context ctx);

    public static native @Cast("size_t") long ggml_used_mem(@Const ggml_context ctx);

    public static native @Cast("size_t") long ggml_set_scratch(ggml_context ctx, @ByVal ggml_scratch scratch);
    public static native @Cast("bool") boolean ggml_get_no_alloc(ggml_context ctx);
    public static native void ggml_set_no_alloc(ggml_context ctx, @Cast("bool") boolean no_alloc);

    public static native Pointer ggml_get_mem_buffer(@Const ggml_context ctx);
    public static native @Cast("size_t") long ggml_get_mem_size(@Const ggml_context ctx);
    public static native @Cast("size_t") long ggml_get_max_tensor_size(@Const ggml_context ctx);

    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") LongPointer ne);
    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") LongBuffer ne);
    public static native ggml_tensor ggml_new_tensor(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                int n_dims,
                @Cast("const int64_t*") long[] ne);

    public static native ggml_tensor ggml_new_tensor_1d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0);

    public static native ggml_tensor ggml_new_tensor_2d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1);

    public static native ggml_tensor ggml_new_tensor_3d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2);

    public static native ggml_tensor ggml_new_tensor_4d(
                ggml_context ctx,
                @Cast("ggml_type") int type,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3);

    public static native ggml_tensor ggml_new_i32(ggml_context ctx, int value);
    public static native ggml_tensor ggml_new_f32(ggml_context ctx, float value);

    public static native ggml_tensor ggml_dup_tensor(ggml_context ctx, @Const ggml_tensor src);
    public static native ggml_tensor ggml_view_tensor(ggml_context ctx, @Const ggml_tensor src);

    public static native ggml_tensor ggml_get_tensor(ggml_context ctx, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_get_tensor(ggml_context ctx, String name);

    public static native ggml_tensor ggml_set_zero(ggml_tensor tensor);
    public static native ggml_tensor ggml_set_i32(ggml_tensor tensor, int value);
    public static native ggml_tensor ggml_set_f32(ggml_tensor tensor, float value);

    public static native int ggml_get_i32_1d(@Const ggml_tensor tensor, int i);
    public static native void ggml_set_i32_1d(@Const ggml_tensor tensor, int i, int value);

    public static native float ggml_get_f32_1d(@Const ggml_tensor tensor, int i);
    public static native void ggml_set_f32_1d(@Const ggml_tensor tensor, int i, float value);

    public static native Pointer ggml_get_data(@Const ggml_tensor tensor);
    public static native FloatPointer ggml_get_data_f32(@Const ggml_tensor tensor);

    public static native @Cast("ggml_unary_op") int ggml_get_unary_op(@Const ggml_tensor tensor);

    public static native @Cast("const char*") BytePointer ggml_get_name(@Const ggml_tensor tensor);
    public static native ggml_tensor ggml_set_name(      ggml_tensor tensor, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_set_name(      ggml_tensor tensor, String name);
    public static native ggml_tensor ggml_format_name(      ggml_tensor tensor, @Cast("const char*") BytePointer fmt);
    public static native ggml_tensor ggml_format_name(      ggml_tensor tensor, String fmt);

    //
    // operations on tensors with backpropagation
    //

    public static native ggml_tensor ggml_dup(
                ggml_context ctx,
                ggml_tensor a);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_dup_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_add(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add1(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_add1_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_acc(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_acc_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_sub(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_sub_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_mul(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_mul_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_div(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_div_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_sqr(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqr_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqrt(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sqrt_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_log(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_log_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // return scalar
    public static native ggml_tensor ggml_sum(
                ggml_context ctx,
                ggml_tensor a);

    // sums along rows, with input shape [a,b,c,d] return shape [1,b,c,d]
    public static native ggml_tensor ggml_sum_rows(
                ggml_context ctx,
                ggml_tensor a);

    // mean along rows
    public static native ggml_tensor ggml_mean(
                ggml_context ctx,
                ggml_tensor a);

    // argmax along rows
    public static native ggml_tensor ggml_argmax(
                ggml_context ctx,
                ggml_tensor a);

    // if a is the same shape as b, and a is not parameter, return a
    // otherwise, return a new tensor: repeat(a) to fit in b
    public static native ggml_tensor ggml_repeat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_repeat_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // concat a and b on dim 2
    // used in stable-diffusion
    public static native ggml_tensor ggml_concat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_abs(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_abs_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sgn(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_sgn_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_neg(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_neg_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_step(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_step_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_tanh(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_tanh_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_elu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_elu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_relu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_relu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // TODO: double-check this computation is correct
    public static native ggml_tensor ggml_gelu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_quick(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_gelu_quick_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_silu(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_silu_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // a - x
    // b - dy
    public static native ggml_tensor ggml_silu_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // normalize along rows
    // TODO: eps is hardcoded to 1e-5 for now
    public static native ggml_tensor ggml_norm(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_norm_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_rms_norm(
                ggml_context ctx,
                ggml_tensor a,
                float eps);

    public static native ggml_tensor ggml_rms_norm_inplace(
                ggml_context ctx,
                ggml_tensor a,
                float eps);

    // group normalize along ne0*ne1*n_groups
    // used in stable-diffusion
    // TODO: eps is hardcoded to 1e-6 for now
    public static native ggml_tensor ggml_group_norm(
                ggml_context ctx,
                ggml_tensor a,
                int n_groups);

    public static native ggml_tensor ggml_group_norm_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_groups);

    // a - x
    // b - dy
    // TODO: update with configurable eps
    public static native ggml_tensor ggml_rms_norm_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // A: n columns, m rows
    // B: n columns, p rows  (i.e. we transpose it internally)
    // result is m columns, p rows
    public static native ggml_tensor ggml_mul_mat(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // A: m columns, n rows,
    // B: p columns, n rows,
    // result is m columns, p rows
    public static native ggml_tensor ggml_out_prod(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    //
    // operations on tensors without backpropagation
    //

    public static native ggml_tensor ggml_scale(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_scale_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // b -> view(a,offset,nb1,nb2,3), return modified a
    public static native ggml_tensor ggml_set(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    // b -> view(a,offset,nb1,nb2,3), return view(a)
    public static native ggml_tensor ggml_set_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_set_1d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_set_1d_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long offset);

    // b -> view(a,offset,nb1,nb2,3), return modified a
    public static native ggml_tensor ggml_set_2d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset);

    // b -> view(a,offset,nb1,nb2,3), return view(a)
    public static native ggml_tensor ggml_set_2d_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset);


    // a -> b, return view(b)
    public static native ggml_tensor ggml_cpy(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // a -> b, in-place, return view(b)
    public static native ggml_tensor ggml_cpy_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // make contiguous
    public static native ggml_tensor ggml_cont(
                ggml_context ctx,
                ggml_tensor a);

    // make contiguous, in-place
    public static native ggml_tensor ggml_cont_inplace(
                ggml_context ctx,
                ggml_tensor a);

    // return view(a), b specifies the new shape
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // return view(a)
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0);

    public static native ggml_tensor ggml_reshape_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1);

    // return view(a)
    // TODO: when we start computing gradient, make a copy instead of view
    public static native ggml_tensor ggml_reshape_3d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2);

    public static native ggml_tensor ggml_reshape_4d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3);

    // offset in bytes
    public static native ggml_tensor ggml_view_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("size_t") long nb1,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_3d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_view_4d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("int64_t") long ne0,
                @Cast("int64_t") long ne1,
                @Cast("int64_t") long ne2,
                @Cast("int64_t") long ne3,
                @Cast("size_t") long nb1,
                @Cast("size_t") long nb2,
                @Cast("size_t") long nb3,
                @Cast("size_t") long offset);

    public static native ggml_tensor ggml_permute(
                ggml_context ctx,
                ggml_tensor a,
                int axis0,
                int axis1,
                int axis2,
                int axis3);

    // alias for ggml_permute(ctx, a, 1, 0, 2, 3)
    public static native ggml_tensor ggml_transpose(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_get_rows(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_get_rows_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c);

    public static native ggml_tensor ggml_diag(
            ggml_context ctx,
            ggml_tensor a);

    // set elements above the diagonal to -INF
    public static native ggml_tensor ggml_diag_mask_inf(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_diag_mask_inf_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // set elements above the diagonal to 0
    public static native ggml_tensor ggml_diag_mask_zero(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_diag_mask_zero_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past);

    public static native ggml_tensor ggml_soft_max(
                ggml_context ctx,
                ggml_tensor a);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_soft_max_inplace(
                ggml_context ctx,
                ggml_tensor a);

    public static native ggml_tensor ggml_soft_max_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_soft_max_back_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // rotary position embedding
    // if mode & 1 == 1, skip n_past elements
    // if mode & 2 == 1, GPT-NeoX style
    // if mode & 4 == 1, ChatGLM style
    // TODO: avoid creating a new tensor every time
    public static native ggml_tensor ggml_rope(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                int mode,
                int n_ctx);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_rope_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                int mode,
                int n_ctx);

    // custom RoPE
    public static native ggml_tensor ggml_rope_custom(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                int mode,
                int n_ctx,
                float freq_base,
                float freq_scale);

    // in-place, returns view(a)
    public static native ggml_tensor ggml_rope_custom_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                int mode,
                int n_ctx,
                float freq_base,
                float freq_scale);

    // xPos RoPE, in-place, returns view(a)
    public static native ggml_tensor ggml_rope_xpos_inplace(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                float base,
                @Cast("bool") boolean down);

    // rotary position embedding backward, i.e compute dx from dy
    // a - dy
    public static native ggml_tensor ggml_rope_back(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_dims,
                int mode,
                int n_ctx,
                float freq_base,
                float freq_scale,
                float xpos_base,
                @Cast("bool") boolean xpos_down);

    // alibi position embedding
    // in-place, returns view(a)
    public static native ggml_tensor ggml_alibi(
                ggml_context ctx,
                ggml_tensor a,
                int n_past,
                int n_head,
                float bias_max);

    // clamp
    // in-place, returns view(a)
    public static native ggml_tensor ggml_clamp(
                ggml_context ctx,
                ggml_tensor a,
                float min,
                float max);

    public static native ggml_tensor ggml_conv_1d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int p0,
                int d0); // dilation

    // conv_1d with padding = half
    // alias for ggml_conv_1d(a, b, s, a->ne[0]/2, d)
    public static native ggml_tensor ggml_conv_1d_ph(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s,
                int d);

    public static native ggml_tensor ggml_conv_2d(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int s0,
                int s1,
                int p0,
                int p1,
                int d0,
                int d1);


    // kernel size is a->ne[0] x a->ne[1]
    // stride is equal to kernel size
    // padding is zero
    // example:
    // a:     16   16    3  768
    // b:   1024 1024    3    1
    // res:   64   64  768    1
    // used in sam
    public static native ggml_tensor ggml_conv_2d_sk_p0(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    // kernel size is a->ne[0] x a->ne[1]
    // stride is 1
    // padding is half
    // example:
    // a:      3    3    256  256
    // b:     64   64    256    1
    // res:   64   64    256    1
    // used in sam
    public static native ggml_tensor ggml_conv_2d_s1_ph(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_conv_transpose_2d_p0(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                int stride);

    /** enum ggml_op_pool */
    public static final int
        GGML_OP_POOL_MAX = 0,
        GGML_OP_POOL_AVG = 1,
        GGML_OP_POOL_COUNT = 2;

    public static native ggml_tensor ggml_pool_1d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("ggml_op_pool") int op,
                int k0,
                int s0,
                int p0); // padding

    public static native ggml_tensor ggml_pool_2d(
                ggml_context ctx,
                ggml_tensor a,
                @Cast("ggml_op_pool") int op,
                int k0,
                int k1,
                int s0,
                int s1,
                int p0,
                int p1);

    // nearest interpolate
    // used in stable-diffusion
    public static native ggml_tensor ggml_upscale(
                ggml_context ctx,
                ggml_tensor a,
                int scale_factor);

    public static native ggml_tensor ggml_flash_attn(
                ggml_context ctx,
                ggml_tensor q,
                ggml_tensor k,
                ggml_tensor v,
                @Cast("bool") boolean masked);

    public static native ggml_tensor ggml_flash_attn_back(
               ggml_context ctx,
               ggml_tensor q,
               ggml_tensor k,
               ggml_tensor v,
               ggml_tensor d,
               @Cast("bool") boolean masked);

    public static native ggml_tensor ggml_flash_ff(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b0,
                ggml_tensor b1,
                ggml_tensor c0,
                ggml_tensor c1);

    // partition into non-overlapping windows with padding if needed
    // example:
    // a:   768   64   64    1
    // w:    14
    // res: 768   14   14    25
    // used in sam
    public static native ggml_tensor ggml_win_part(
                ggml_context ctx,
                ggml_tensor a,
                int w);

    // reverse of ggml_win_part
    // used in sam
    public static native ggml_tensor ggml_win_unpart(
                ggml_context ctx,
                ggml_tensor a,
                int w0,
                int h0,
                int w);

    public static native ggml_tensor ggml_unary(
                ggml_context ctx,
                 ggml_tensor a,
                 @Cast("ggml_unary_op") int op);

    public static native ggml_tensor ggml_unary_inplace(
            ggml_context ctx,
            ggml_tensor a,
            @Cast("ggml_unary_op") int op);

    // used in sam
    public static native ggml_tensor ggml_get_rel_pos(
                ggml_context ctx,
                ggml_tensor a,
                int qh,
                int kh);

    // used in sam

    public static native ggml_tensor ggml_add_rel_pos(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor pw,
                ggml_tensor ph);

    public static native ggml_tensor ggml_add_rel_pos_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor pw,
                ggml_tensor ph);
// Targeting ../ggml_unary_op_f32_t.java


// Targeting ../ggml_binary_op_f32_t.java


// Targeting ../ggml_custom1_op_f32_t.java


// Targeting ../ggml_custom2_op_f32_t.java


// Targeting ../ggml_custom3_op_f32_t.java



    public static native ggml_tensor ggml_map_unary_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_unary_op_f32_t fun);

    public static native ggml_tensor ggml_map_unary_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_unary_op_f32_t fun);

    public static native ggml_tensor ggml_map_binary_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_binary_op_f32_t fun);

    public static native ggml_tensor ggml_map_binary_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_binary_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom1_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_custom1_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom1_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                       ggml_custom1_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom2_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_custom2_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom2_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                       ggml_custom2_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom3_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                       ggml_custom3_op_f32_t fun);

    public static native ggml_tensor ggml_map_custom3_inplace_f32(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                       ggml_custom3_op_f32_t fun);
// Targeting ../ggml_custom1_op_t.java


// Targeting ../ggml_custom2_op_t.java


// Targeting ../ggml_custom3_op_t.java



    public static final int GGML_N_TASKS_MAX = -1;

    public static native ggml_tensor ggml_map_custom1(
                ggml_context ctx,
                ggml_tensor a,
                ggml_custom1_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom1_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_custom1_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom2(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_custom2_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom2_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_custom2_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom3(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                ggml_custom3_op_t fun,
                int n_tasks,
                Pointer userdata);

    public static native ggml_tensor ggml_map_custom3_inplace(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c,
                ggml_custom3_op_t fun,
                int n_tasks,
                Pointer userdata);

    // loss function

    public static native ggml_tensor ggml_cross_entropy_loss(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b);

    public static native ggml_tensor ggml_cross_entropy_loss_back(
                ggml_context ctx,
                ggml_tensor a,
                ggml_tensor b,
                ggml_tensor c);

    //
    // automatic differentiation
    //

    public static native void ggml_set_param(
                ggml_context ctx,
                ggml_tensor tensor);


    public static native void ggml_build_forward_expand(ggml_cgraph cgraph, ggml_tensor tensor);

    public static native @ByVal ggml_cgraph ggml_build_forward(ggml_tensor tensor);
    public static native @ByVal ggml_cgraph ggml_build_backward(ggml_context ctx, ggml_cgraph gf, @Cast("bool") boolean keep);

    // graph allocation in a context
    public static native ggml_cgraph ggml_new_graph(ggml_context ctx);
    public static native ggml_cgraph ggml_build_forward_ctx(ggml_context ctx, ggml_tensor tensor);
    public static native @Cast("size_t") long ggml_graph_overhead();

    // ggml_graph_plan() has to be called before ggml_graph_compute()
    // when plan.work_size > 0, caller must allocate memory for plan.work_data
    public static native @ByVal ggml_cplan ggml_graph_plan(ggml_cgraph cgraph, int n_threads);
    public static native int ggml_graph_compute(ggml_cgraph cgraph, ggml_cplan cplan);
    public static native void ggml_graph_reset(ggml_cgraph cgraph);

    // same as ggml_graph_compute() but the work data is allocated as a part of the context
    // note: the drawback of this API is that you must have ensured that the context has enough memory for the work data
    public static native void ggml_graph_compute_with_ctx(ggml_context ctx, ggml_cgraph cgraph, int n_threads);

    public static native ggml_tensor ggml_graph_get_tensor(ggml_cgraph cgraph, @Cast("const char*") BytePointer name);
    public static native ggml_tensor ggml_graph_get_tensor(ggml_cgraph cgraph, String name);

    public static native void ggml_graph_export(@Const ggml_cgraph cgraph, @Cast("const char*") BytePointer fname);
    public static native void ggml_graph_export(@Const ggml_cgraph cgraph, String fname);
    public static native @ByVal ggml_cgraph ggml_graph_import(@Cast("const char*") BytePointer fname, @Cast("ggml_context**") PointerPointer ctx_data, @Cast("ggml_context**") PointerPointer ctx_eval);
    public static native @ByVal ggml_cgraph ggml_graph_import(@Cast("const char*") BytePointer fname, @ByPtrPtr ggml_context ctx_data, @ByPtrPtr ggml_context ctx_eval);
    public static native @ByVal ggml_cgraph ggml_graph_import(String fname, @ByPtrPtr ggml_context ctx_data, @ByPtrPtr ggml_context ctx_eval);

    // print info and performance information for the graph
    public static native void ggml_graph_print(@Const ggml_cgraph cgraph);

    // dump the graph into a file using the dot format
    public static native void ggml_graph_dump_dot(@Const ggml_cgraph gb, @Const ggml_cgraph gf, @Cast("const char*") BytePointer filename);
    public static native void ggml_graph_dump_dot(@Const ggml_cgraph gb, @Const ggml_cgraph gf, String filename);

    //
    // optimization
    //

    // optimization methods
    /** enum ggml_opt_type */
    public static final int
        GGML_OPT_ADAM = 0,
        GGML_OPT_LBFGS = 1;

    // linesearch methods
    /** enum ggml_linesearch */
    public static final int
        GGML_LINESEARCH_DEFAULT = 1,

        GGML_LINESEARCH_BACKTRACKING_ARMIJO       = 0,
        GGML_LINESEARCH_BACKTRACKING_WOLFE        = 1,
        GGML_LINESEARCH_BACKTRACKING_STRONG_WOLFE = 2;

    // optimization return values
    /** enum ggml_opt_result */
    public static final int
        GGML_OPT_OK = 0,
        GGML_OPT_DID_NOT_CONVERGE = 1,
        GGML_OPT_NO_CONTEXT = 2,
        GGML_OPT_INVALID_WOLFE = 3,
        GGML_OPT_FAIL = 4,

        GGML_LINESEARCH_FAIL = -128,
        GGML_LINESEARCH_MINIMUM_STEP = -127,
        GGML_LINESEARCH_MAXIMUM_STEP = -126,
        GGML_LINESEARCH_MAXIMUM_ITERATIONS = -125,
        GGML_LINESEARCH_INVALID_PARAMETERS = -124;
// Targeting ../ggml_opt_params.java


// Targeting ../ggml_opt_context.java



    public static native @ByVal ggml_opt_params ggml_opt_default_params(@Cast("ggml_opt_type") int type);

    // optimize the function defined by the tensor f
    public static native @Cast("ggml_opt_result") int ggml_opt(
                ggml_context ctx,
                @ByVal ggml_opt_params params,
                ggml_tensor f);

    // initialize optimizer context
    public static native void ggml_opt_init(
                ggml_context ctx,
                ggml_opt_context opt,
                @ByVal ggml_opt_params params,
                @Cast("int64_t") long nx);

    // continue optimizing the function defined by the tensor f
    public static native @Cast("ggml_opt_result") int ggml_opt_resume(
                ggml_context ctx,
                ggml_opt_context opt,
                ggml_tensor f);

    // continue optimizing the function defined by the tensor f
    public static native @Cast("ggml_opt_result") int ggml_opt_resume_g(
                ggml_context ctx,
                ggml_opt_context opt,
                ggml_tensor f,
                ggml_cgraph gf,
                ggml_cgraph gb);

    //
    // quantization
    //

    public static native @Cast("size_t") long ggml_quantize_q4_0(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q4_0(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q4_0(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
    public static native @Cast("size_t") long ggml_quantize_q4_1(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q4_1(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q4_1(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
    public static native @Cast("size_t") long ggml_quantize_q5_0(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q5_0(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q5_0(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
    public static native @Cast("size_t") long ggml_quantize_q5_1(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q5_1(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q5_1(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);
    public static native @Cast("size_t") long ggml_quantize_q8_0(@Const FloatPointer src, Pointer dst, int n, int k, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_q8_0(@Const FloatBuffer src, Pointer dst, int n, int k, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_q8_0(@Const float[] src, Pointer dst, int n, int k, @Cast("int64_t*") long[] hist);

    public static native @Cast("size_t") long ggml_quantize_chunk(@Cast("ggml_type") int type, @Const FloatPointer src, Pointer dst, int start, int n, @Cast("int64_t*") LongPointer hist);
    public static native @Cast("size_t") long ggml_quantize_chunk(@Cast("ggml_type") int type, @Const FloatBuffer src, Pointer dst, int start, int n, @Cast("int64_t*") LongBuffer hist);
    public static native @Cast("size_t") long ggml_quantize_chunk(@Cast("ggml_type") int type, @Const float[] src, Pointer dst, int start, int n, @Cast("int64_t*") long[] hist);

    //
    // gguf
    //

    /** enum gguf_type */
    public static final int
        GGUF_TYPE_UINT8   = 0,
        GGUF_TYPE_INT8    = 1,
        GGUF_TYPE_UINT16  = 2,
        GGUF_TYPE_INT16   = 3,
        GGUF_TYPE_UINT32  = 4,
        GGUF_TYPE_INT32   = 5,
        GGUF_TYPE_FLOAT32 = 6,
        GGUF_TYPE_BOOL    = 7,
        GGUF_TYPE_STRING  = 8,
        GGUF_TYPE_ARRAY   = 9,
        GGUF_TYPE_COUNT = 10;       // marks the end of the enum
// Targeting ../gguf_context.java


// Targeting ../gguf_init_params.java



    public static native gguf_context gguf_init_empty();
    public static native gguf_context gguf_init_from_file(@Cast("const char*") BytePointer fname, @ByVal gguf_init_params params);
    public static native gguf_context gguf_init_from_file(String fname, @ByVal gguf_init_params params);
    //GGML_API struct gguf_context * gguf_init_from_buffer(..);

    public static native void gguf_free(gguf_context ctx);

    public static native @Cast("const char*") BytePointer gguf_type_name(@Cast("gguf_type") int type);

    public static native int gguf_get_version(gguf_context ctx);
    public static native @Cast("size_t") long gguf_get_alignment(gguf_context ctx);
    public static native @Cast("size_t") long gguf_get_data_offset(gguf_context ctx);
    public static native Pointer gguf_get_data(gguf_context ctx);

    public static native int gguf_get_n_kv(gguf_context ctx);
    public static native int gguf_find_key(gguf_context ctx, @Cast("const char*") BytePointer key);
    public static native int gguf_find_key(gguf_context ctx, String key);
    public static native @Cast("const char*") BytePointer gguf_get_key(gguf_context ctx, int i);

    public static native @Cast("gguf_type") int gguf_get_kv_type(gguf_context ctx, int i);
    public static native @Cast("gguf_type") int gguf_get_arr_type(gguf_context ctx, int i);

    // results are undefined if the wrong type is used for the key
    public static native @Cast("uint8_t") byte gguf_get_val_u8(gguf_context ctx, int i);
    public static native byte gguf_get_val_i8(gguf_context ctx, int i);
    public static native @Cast("uint16_t") short gguf_get_val_u16(gguf_context ctx, int i);
    public static native short gguf_get_val_i16(gguf_context ctx, int i);
    public static native @Cast("uint32_t") int gguf_get_val_u32(gguf_context ctx, int i);
    public static native int gguf_get_val_i32(gguf_context ctx, int i);
    public static native float gguf_get_val_f32(gguf_context ctx, int i);
    public static native @Cast("bool") boolean gguf_get_val_bool(gguf_context ctx, int i);
    public static native @Cast("const char*") BytePointer gguf_get_val_str(gguf_context ctx, int i);
    public static native int gguf_get_arr_n(gguf_context ctx, int i);
    public static native @Const Pointer gguf_get_arr_data(gguf_context ctx, int i);
    public static native @Cast("const char*") BytePointer gguf_get_arr_str(gguf_context ctx, int key_id, int i);

    public static native int gguf_get_n_tensors(gguf_context ctx);
    public static native int gguf_find_tensor(gguf_context ctx, @Cast("const char*") BytePointer name);
    public static native int gguf_find_tensor(gguf_context ctx, String name);
    public static native @Cast("size_t") long gguf_get_tensor_offset(gguf_context ctx, int i);
    public static native @Cast("char*") BytePointer gguf_get_tensor_name(gguf_context ctx, int i);

    // overrides existing values or adds a new one
    public static native void gguf_set_val_u8(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("uint8_t") byte val);
    public static native void gguf_set_val_u8(gguf_context ctx, String key, @Cast("uint8_t") byte val);
    public static native void gguf_set_val_i8(gguf_context ctx, @Cast("const char*") BytePointer key, byte val);
    public static native void gguf_set_val_i8(gguf_context ctx, String key, byte val);
    public static native void gguf_set_val_u16(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("uint16_t") short val);
    public static native void gguf_set_val_u16(gguf_context ctx, String key, @Cast("uint16_t") short val);
    public static native void gguf_set_val_i16(gguf_context ctx, @Cast("const char*") BytePointer key, short val);
    public static native void gguf_set_val_i16(gguf_context ctx, String key, short val);
    public static native void gguf_set_val_u32(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("uint32_t") int val);
    public static native void gguf_set_val_u32(gguf_context ctx, String key, @Cast("uint32_t") int val);
    public static native void gguf_set_val_i32(gguf_context ctx, @Cast("const char*") BytePointer key, int val);
    public static native void gguf_set_val_i32(gguf_context ctx, String key, int val);
    public static native void gguf_set_val_f32(gguf_context ctx, @Cast("const char*") BytePointer key, float val);
    public static native void gguf_set_val_f32(gguf_context ctx, String key, float val);
    public static native void gguf_set_val_bool(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("bool") boolean val);
    public static native void gguf_set_val_bool(gguf_context ctx, String key, @Cast("bool") boolean val);
    public static native void gguf_set_val_str(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("const char*") BytePointer val);
    public static native void gguf_set_val_str(gguf_context ctx, String key, String val);
    public static native void gguf_set_arr_data(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("gguf_type") int type, @Const Pointer data, int n);
    public static native void gguf_set_arr_data(gguf_context ctx, String key, @Cast("gguf_type") int type, @Const Pointer data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("const char**") PointerPointer data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("const char**") @ByPtrPtr BytePointer data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, String key, @Cast("const char**") @ByPtrPtr ByteBuffer data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("const char**") @ByPtrPtr byte[] data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, String key, @Cast("const char**") @ByPtrPtr BytePointer data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, @Cast("const char*") BytePointer key, @Cast("const char**") @ByPtrPtr ByteBuffer data, int n);
    public static native void gguf_set_arr_str(gguf_context ctx, String key, @Cast("const char**") @ByPtrPtr byte[] data, int n);

    // set or add KV pairs from another context
    public static native void gguf_set_kv(gguf_context ctx, gguf_context src);

    // manage tensor info
    public static native void gguf_add_tensor(gguf_context ctx, @Const ggml_tensor tensor);
    public static native void gguf_set_tensor_type(gguf_context ctx, @Cast("const char*") BytePointer name, @Cast("ggml_type") int type);
    public static native void gguf_set_tensor_type(gguf_context ctx, String name, @Cast("ggml_type") int type);
    public static native void gguf_set_tensor_data(gguf_context ctx, @Cast("const char*") BytePointer name, @Const Pointer data, @Cast("size_t") long size);
    public static native void gguf_set_tensor_data(gguf_context ctx, String name, @Const Pointer data, @Cast("size_t") long size);

    // writing gguf files can be done in 2 ways:
    //
    // - write the entire gguf_context to a binary file in a single pass:
    //
    //   gguf_write_to_file(ctx, fname);
    //
    // - first prepare a file with a placeholder for the meta data, write the tensor data, then write the meta data:
    //
    //   FILE * f = fopen(fname, "wb");
    //   fseek(f, gguf_get_meta_size(ctx), SEEK_SET);
    //   fwrite(f, ...);
    //   void * data = gguf_meta_get_meta_data(ctx);
    //   fseek(f, 0, SEEK_SET);
    //   fwrite(f, data, gguf_get_meta_size(ctx));
    //   free(data);
    //   fclose(f);
    //

    // write the entire context to a binary file
    public static native void gguf_write_to_file(gguf_context ctx, @Cast("const char*") BytePointer fname, @Cast("bool") boolean only_meta);
    public static native void gguf_write_to_file(gguf_context ctx, String fname, @Cast("bool") boolean only_meta);

    // get the size in bytes of the meta data (header, kv pairs, tensor info) including padding
    public static native @Cast("size_t") long gguf_get_meta_size(gguf_context ctx);
    public static native void gguf_get_meta_data(gguf_context ctx, Pointer data);

    //
    // system info
    //

    public static native int ggml_cpu_has_avx();
    public static native int ggml_cpu_has_avx2();
    public static native int ggml_cpu_has_avx512();
    public static native int ggml_cpu_has_avx512_vbmi();
    public static native int ggml_cpu_has_avx512_vnni();
    public static native int ggml_cpu_has_fma();
    public static native int ggml_cpu_has_neon();
    public static native int ggml_cpu_has_arm_fma();
    public static native int ggml_cpu_has_f16c();
    public static native int ggml_cpu_has_fp16_va();
    public static native int ggml_cpu_has_wasm_simd();
    public static native int ggml_cpu_has_blas();
    public static native int ggml_cpu_has_cublas();
    public static native int ggml_cpu_has_clblast();
    public static native int ggml_cpu_has_gpublas();
    public static native int ggml_cpu_has_sse3();
    public static native int ggml_cpu_has_vsx();

    //
    // Internal types and functions exposed for tests and benchmarks
    //

// #ifdef  __cplusplus
// restrict not standard in C++
// #define GGML_RESTRICT
// Targeting ../ggml_to_float_t.java


// Targeting ../ggml_from_float_t.java


// Targeting ../ggml_vec_dot_t.java


// Targeting ../ggml_type_traits_t.java



    public static native @ByVal ggml_type_traits_t ggml_internal_get_type_traits(@Cast("ggml_type") int type);

// #ifdef  __cplusplus
// #endif


// Parsed from llama.h

// #ifndef LLAMA_H
// #define LLAMA_H

// #include "ggml.h"
// #ifdef GGML_USE_CUBLAS
// #else
public static final int LLAMA_MAX_DEVICES = 1;
// #endif // GGML_USE_CUBLAS
// #include <stddef.h>
// #include <stdint.h>
// #include <stdbool.h>

// #ifdef LLAMA_SHARED
// #else
// #    define LLAMA_API
// #endif

// #ifdef __GNUC__
// #    define DEPRECATED(func, hint) func __attribute__((deprecated(hint)))
// #elif defined(_MSC_VER)
// #    define DEPRECATED(func, hint) __declspec(deprecated(hint)) func
// #else
// #    define DEPRECATED(func, hint) func
// #endif

public static final int LLAMA_DEFAULT_SEED = 0xFFFFFFFF;

public static final int LLAMA_FILE_MAGIC_GGSN = 0x6767736e; // 'ggsn'

public static final int LLAMA_SESSION_MAGIC =   LLAMA_FILE_MAGIC_GGSN;
public static final int LLAMA_SESSION_VERSION = 1;

// #if defined(GGML_USE_CUBLAS) || defined(GGML_USE_CLBLAST) || defined(GGML_USE_METAL)
// Defined when llama.cpp is compiled with support for offloading model layers to GPU.
// #define LLAMA_SUPPORTS_GPU_OFFLOAD
// #endif

// #ifdef __cplusplus
// Targeting ../llama_model.java


// Targeting ../llama_context.java



    /** enum llama_log_level */
    public static final int
        LLAMA_LOG_LEVEL_ERROR = 2,
        LLAMA_LOG_LEVEL_WARN  = 3,
        LLAMA_LOG_LEVEL_INFO  = 4;

    /** enum llama_vocab_type */
    public static final int
        LLAMA_VOCAB_TYPE_SPM = 0, // SentencePiece
        LLAMA_VOCAB_TYPE_BPE = 1; // Byte Pair Encoding

    /** enum llama_token_type */
    public static final int
        LLAMA_TOKEN_TYPE_UNDEFINED    = 0,
        LLAMA_TOKEN_TYPE_NORMAL       = 1,
        LLAMA_TOKEN_TYPE_UNKNOWN      = 2,
        LLAMA_TOKEN_TYPE_CONTROL      = 3,
        LLAMA_TOKEN_TYPE_USER_DEFINED = 4,
        LLAMA_TOKEN_TYPE_UNUSED       = 5,
        LLAMA_TOKEN_TYPE_BYTE         = 6;

    // model file types
    /** enum llama_ftype */
    public static final int
        LLAMA_FTYPE_ALL_F32              = 0,
        LLAMA_FTYPE_MOSTLY_F16           = 1, // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_0          = 2, // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_1          = 3, // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4, // tok_embeddings.weight and output.weight are F16
        // LLAMA_FTYPE_MOSTLY_Q4_2       = 5, // support has been removed
        // LLAMA_FTYPE_MOSTLY_Q4_3       = 6, // support has been removed
        LLAMA_FTYPE_MOSTLY_Q8_0          = 7, // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q5_0          = 8, // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q5_1          = 9, // except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q2_K          = 10,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q3_K_S        = 11,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q3_K_M        = 12,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q3_K_L        = 13,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_K_S        = 14,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q4_K_M        = 15,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q5_K_S        = 16,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q5_K_M        = 17,// except 1d tensors
        LLAMA_FTYPE_MOSTLY_Q6_K          = 18,// except 1d tensors

        LLAMA_FTYPE_GUESSED = 1024; // not specified in the model file
// Targeting ../llama_token_data.java


// Targeting ../llama_token_data_array.java


// Targeting ../llama_progress_callback.java


// Targeting ../llama_context_params.java


// Targeting ../llama_log_callback.java


// Targeting ../llama_model_quantize_params.java


// Targeting ../llama_grammar.java



    // grammar element type
    /** enum llama_gretype */
    public static final int
        // end of rule definition
        LLAMA_GRETYPE_END            = 0,

        // start of alternate definition for rule
        LLAMA_GRETYPE_ALT            = 1,

        // non-terminal element: reference to rule
        LLAMA_GRETYPE_RULE_REF       = 2,

        // terminal element: character (code point)
        LLAMA_GRETYPE_CHAR           = 3,

        // inverse char(s) ([^a], [^a-b] [^abc])
        LLAMA_GRETYPE_CHAR_NOT       = 4,

        // modifies a preceding LLAMA_GRETYPE_CHAR or LLAMA_GRETYPE_CHAR_ALT to
        // be an inclusive range ([a-z])
        LLAMA_GRETYPE_CHAR_RNG_UPPER = 5,

        // modifies a preceding LLAMA_GRETYPE_CHAR or
        // LLAMA_GRETYPE_CHAR_RNG_UPPER to add an alternate char to match ([ab], [a-zA])
        LLAMA_GRETYPE_CHAR_ALT       = 6;
// Targeting ../llama_grammar_element.java


// Targeting ../llama_timings.java



    public static native @ByVal llama_context_params llama_context_default_params();
    public static native @ByVal llama_model_quantize_params llama_model_quantize_default_params();

    // Initialize the llama + ggml backend
    // If numa is true, use NUMA optimizations
    // Call once at the start of the program
    public static native void llama_backend_init(@Cast("bool") boolean numa);

    // Call once at the end of the program - currently only used for MPI
    public static native void llama_backend_free();

    public static native llama_model llama_load_model_from_file(
                                 @Cast("const char*") BytePointer path_model,
                @ByVal llama_context_params params);
    public static native llama_model llama_load_model_from_file(
                                 String path_model,
                @ByVal llama_context_params params);

    public static native void llama_free_model(llama_model model);

    public static native llama_context llama_new_context_with_model(
                         llama_model model,
                @ByVal llama_context_params params);

    // Frees all allocated memory
    public static native void llama_free(llama_context ctx);

    public static native @Cast("int64_t") long llama_time_us();

    public static native int llama_max_devices();
    public static native @Cast("bool") boolean llama_mmap_supported();
    public static native @Cast("bool") boolean llama_mlock_supported();

    public static native int llama_n_vocab(@Const llama_context ctx);
    public static native int llama_n_ctx(@Const llama_context ctx);
    public static native int llama_n_embd(@Const llama_context ctx);

    public static native int llama_model_n_vocab(@Const llama_model model);
    public static native int llama_model_n_ctx(@Const llama_model model);
    public static native int llama_model_n_embd(@Const llama_model model);

    // Get a string describing the model type
    public static native int llama_model_type(@Const llama_model model, @Cast("char*") BytePointer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_type(@Const llama_model model, @Cast("char*") ByteBuffer buf, @Cast("size_t") long buf_size);
    public static native int llama_model_type(@Const llama_model model, @Cast("char*") byte[] buf, @Cast("size_t") long buf_size);

    // Returns 0 on success
    public static native int llama_model_quantize(
                @Cast("const char*") BytePointer fname_inp,
                @Cast("const char*") BytePointer fname_out,
                @Const llama_model_quantize_params params);
    public static native int llama_model_quantize(
                String fname_inp,
                String fname_out,
                @Const llama_model_quantize_params params);

    // Apply a LoRA adapter to a loaded model
    // path_base_model is the path to a higher quality model to use as a base for
    // the layers modified by the adapter. Can be NULL to use the current loaded model.
    // The model needs to be reloaded before applying a new adapter, otherwise the adapter
    // will be applied on top of the previous one
    // Returns 0 on success
    

    public static native int llama_model_apply_lora_from_file(
                @Const llama_model model,
                              @Cast("const char*") BytePointer path_lora,
                              @Cast("const char*") BytePointer path_base_model,
                                     int n_threads);
    public static native int llama_model_apply_lora_from_file(
                @Const llama_model model,
                              String path_lora,
                              String path_base_model,
                                     int n_threads);

    // Returns the number of tokens in the KV cache
    public static native int llama_get_kv_cache_token_count(@Const llama_context ctx);

    // Sets the current rng seed.
    public static native void llama_set_rng_seed(llama_context ctx, @Cast("uint32_t") int seed);

    // Returns the maximum size in bytes of the state (rng, logits, embedding
    // and kv_cache) - will often be smaller after compacting tokens
    public static native @Cast("size_t") long llama_get_state_size(@Const llama_context ctx);

    // Copies the state to the specified destination address.
    // Destination needs to have allocated enough memory.
    // Returns the number of bytes copied
    public static native @Cast("size_t") long llama_copy_state_data(llama_context ctx, @Cast("uint8_t*") BytePointer dst);
    public static native @Cast("size_t") long llama_copy_state_data(llama_context ctx, @Cast("uint8_t*") ByteBuffer dst);
    public static native @Cast("size_t") long llama_copy_state_data(llama_context ctx, @Cast("uint8_t*") byte[] dst);

    // Set the state reading from the specified address
    // Returns the number of bytes read
    public static native @Cast("size_t") long llama_set_state_data(llama_context ctx, @Cast("uint8_t*") BytePointer src);
    public static native @Cast("size_t") long llama_set_state_data(llama_context ctx, @Cast("uint8_t*") ByteBuffer src);
    public static native @Cast("size_t") long llama_set_state_data(llama_context ctx, @Cast("uint8_t*") byte[] src);

    // Save/load session file
    public static native @Cast("bool") boolean llama_load_session_file(llama_context ctx, @Cast("const char*") BytePointer path_session, @Cast("llama_token*") IntPointer tokens_out, @Cast("size_t") long n_token_capacity, @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(llama_context ctx, String path_session, @Cast("llama_token*") IntBuffer tokens_out, @Cast("size_t") long n_token_capacity, @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(llama_context ctx, @Cast("const char*") BytePointer path_session, @Cast("llama_token*") int[] tokens_out, @Cast("size_t") long n_token_capacity, @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(llama_context ctx, String path_session, @Cast("llama_token*") IntPointer tokens_out, @Cast("size_t") long n_token_capacity, @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(llama_context ctx, @Cast("const char*") BytePointer path_session, @Cast("llama_token*") IntBuffer tokens_out, @Cast("size_t") long n_token_capacity, @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_load_session_file(llama_context ctx, String path_session, @Cast("llama_token*") int[] tokens_out, @Cast("size_t") long n_token_capacity, @Cast("size_t*") SizeTPointer n_token_count_out);
    public static native @Cast("bool") boolean llama_save_session_file(llama_context ctx, @Cast("const char*") BytePointer path_session, @Cast("const llama_token*") IntPointer tokens, @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(llama_context ctx, String path_session, @Cast("const llama_token*") IntBuffer tokens, @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(llama_context ctx, @Cast("const char*") BytePointer path_session, @Cast("const llama_token*") int[] tokens, @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(llama_context ctx, String path_session, @Cast("const llama_token*") IntPointer tokens, @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(llama_context ctx, @Cast("const char*") BytePointer path_session, @Cast("const llama_token*") IntBuffer tokens, @Cast("size_t") long n_token_count);
    public static native @Cast("bool") boolean llama_save_session_file(llama_context ctx, String path_session, @Cast("const llama_token*") int[] tokens, @Cast("size_t") long n_token_count);

    // Run the llama inference to obtain the logits and probabilities for the next token.
    // tokens + n_tokens is the provided batch of new tokens to process
    // n_past is the number of tokens to use from previous eval calls
    // Returns 0 on success
    public static native int llama_eval(
                llama_context ctx,
                   @Cast("const llama_token*") IntPointer tokens,
                                 int n_tokens,
                                 int n_past,
                                 int n_threads);
    public static native int llama_eval(
                llama_context ctx,
                   @Cast("const llama_token*") IntBuffer tokens,
                                 int n_tokens,
                                 int n_past,
                                 int n_threads);
    public static native int llama_eval(
                llama_context ctx,
                   @Cast("const llama_token*") int[] tokens,
                                 int n_tokens,
                                 int n_past,
                                 int n_threads);

    // Same as llama_eval, but use float matrix input directly.
    public static native int llama_eval_embd(
                llama_context ctx,
                         @Const FloatPointer embd,
                                 int n_tokens,
                                 int n_past,
                                 int n_threads);
    public static native int llama_eval_embd(
                llama_context ctx,
                         @Const FloatBuffer embd,
                                 int n_tokens,
                                 int n_past,
                                 int n_threads);
    public static native int llama_eval_embd(
                llama_context ctx,
                         @Const float[] embd,
                                 int n_tokens,
                                 int n_past,
                                 int n_threads);

    // Export a static computation graph for context of 511 and batch size of 1
    // NOTE: since this functionality is mostly for debugging and demonstration purposes, we hardcode these
    //       parameters here to keep things simple
    // IMPORTANT: do not use for anything else other than debugging and testing!
    public static native int llama_eval_export(llama_context ctx, @Cast("const char*") BytePointer fname);
    public static native int llama_eval_export(llama_context ctx, String fname);

    // Token logits obtained from the last call to llama_eval()
    // The logits for the last token are stored in the last row
    // Can be mutated in order to change the probabilities of the next token
    // Rows: n_tokens
    // Cols: n_vocab
    public static native FloatPointer llama_get_logits(llama_context ctx);

    // Get the embeddings for the input
    // shape: [n_embd] (1-dimensional)
    public static native FloatPointer llama_get_embeddings(llama_context ctx);

    //
    // Vocab
    //

    public static native @Cast("const char*") BytePointer llama_token_get_text(@Const llama_context ctx, @Cast("llama_token") int token);

    public static native float llama_token_get_score(@Const llama_context ctx, @Cast("llama_token") int token);

    public static native @Cast("llama_token_type") int llama_token_get_type(@Const llama_context ctx, @Cast("llama_token") int token);

    // Special tokens
    public static native @Cast("llama_token") int llama_token_bos(@Const llama_context ctx);  // beginning-of-sentence
    public static native @Cast("llama_token") int llama_token_eos(@Const llama_context ctx);  // end-of-sentence
    public static native @Cast("llama_token") int llama_token_nl(@Const llama_context ctx);  // next-line

    //
    // Tokenization
    //

    // Convert the provided text into tokens.
    // The tokens pointer must be large enough to hold the resulting tokens.
    // Returns the number of tokens on success, no more than n_max_tokens
    // Returns a negative number on failure - the number of tokens that would have been returned
    public static native int llama_tokenize(
                llama_context ctx,
                          @Cast("const char*") BytePointer text,
                         @Cast("llama_token*") IntPointer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize(
                llama_context ctx,
                          String text,
                         @Cast("llama_token*") IntBuffer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize(
                llama_context ctx,
                          @Cast("const char*") BytePointer text,
                         @Cast("llama_token*") int[] tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize(
                llama_context ctx,
                          String text,
                         @Cast("llama_token*") IntPointer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize(
                llama_context ctx,
                          @Cast("const char*") BytePointer text,
                         @Cast("llama_token*") IntBuffer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize(
                llama_context ctx,
                          String text,
                         @Cast("llama_token*") int[] tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);

    public static native int llama_tokenize_bpe(
                llama_context ctx,
                          @Cast("const char*") BytePointer text,
                         @Cast("llama_token*") IntPointer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize_bpe(
                llama_context ctx,
                          String text,
                         @Cast("llama_token*") IntBuffer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize_bpe(
                llama_context ctx,
                          @Cast("const char*") BytePointer text,
                         @Cast("llama_token*") int[] tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize_bpe(
                llama_context ctx,
                          String text,
                         @Cast("llama_token*") IntPointer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize_bpe(
                llama_context ctx,
                          @Cast("const char*") BytePointer text,
                         @Cast("llama_token*") IntBuffer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize_bpe(
                llama_context ctx,
                          String text,
                         @Cast("llama_token*") int[] tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);

    public static native int llama_tokenize_with_model(
            @Const llama_model model,
                          @Cast("const char*") BytePointer text,
                         @Cast("llama_token*") IntPointer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize_with_model(
            @Const llama_model model,
                          String text,
                         @Cast("llama_token*") IntBuffer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize_with_model(
            @Const llama_model model,
                          @Cast("const char*") BytePointer text,
                         @Cast("llama_token*") int[] tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize_with_model(
            @Const llama_model model,
                          String text,
                         @Cast("llama_token*") IntPointer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize_with_model(
            @Const llama_model model,
                          @Cast("const char*") BytePointer text,
                         @Cast("llama_token*") IntBuffer tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);
    public static native int llama_tokenize_with_model(
            @Const llama_model model,
                          String text,
                         @Cast("llama_token*") int[] tokens,
                                 int n_max_tokens,
                                @Cast("bool") boolean add_bos);

    // Token Id -> String. Uses the vocabulary in the provided context
    // Does not write null terminator to the buffer
    public static native int llama_token_to_str(
                @Const llama_context ctx,
                               @Cast("llama_token") int token,
                                      @Cast("char*") BytePointer buf,
                                      int length);
    public static native int llama_token_to_str(
                @Const llama_context ctx,
                               @Cast("llama_token") int token,
                                      @Cast("char*") ByteBuffer buf,
                                      int length);
    public static native int llama_token_to_str(
                @Const llama_context ctx,
                               @Cast("llama_token") int token,
                                      @Cast("char*") byte[] buf,
                                      int length);

    public static native int llama_token_to_str_bpe(
                @Const llama_context ctx,
                               @Cast("llama_token") int token,
                                      @Cast("char*") BytePointer buf,
                                      int length);
    public static native int llama_token_to_str_bpe(
                @Const llama_context ctx,
                               @Cast("llama_token") int token,
                                      @Cast("char*") ByteBuffer buf,
                                      int length);
    public static native int llama_token_to_str_bpe(
                @Const llama_context ctx,
                               @Cast("llama_token") int token,
                                      @Cast("char*") byte[] buf,
                                      int length);

    public static native int llama_token_to_str_with_model(
                  @Const llama_model model,
                               @Cast("llama_token") int token,
                                      @Cast("char*") BytePointer buf,
                                      int length);
    public static native int llama_token_to_str_with_model(
                  @Const llama_model model,
                               @Cast("llama_token") int token,
                                      @Cast("char*") ByteBuffer buf,
                                      int length);
    public static native int llama_token_to_str_with_model(
                  @Const llama_model model,
                               @Cast("llama_token") int token,
                                      @Cast("char*") byte[] buf,
                                      int length);

    //
    // Grammar
    //

    public static native llama_grammar llama_grammar_init(
                @Cast("const llama_grammar_element**") PointerPointer rules,
                                     @Cast("size_t") long n_rules,
                                     @Cast("size_t") long start_rule_index);
    public static native llama_grammar llama_grammar_init(
                @Const @ByPtrPtr llama_grammar_element rules,
                                     @Cast("size_t") long n_rules,
                                     @Cast("size_t") long start_rule_index);

    public static native void llama_grammar_free(llama_grammar grammar);

    //
    // Sampling functions
    //

    /** \details Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix. */
    public static native void llama_sample_repetition_penalty(llama_context ctx, llama_token_data_array candidates, @Cast("const llama_token*") IntPointer last_tokens, @Cast("size_t") long last_tokens_size, float penalty);
    public static native void llama_sample_repetition_penalty(llama_context ctx, llama_token_data_array candidates, @Cast("const llama_token*") IntBuffer last_tokens, @Cast("size_t") long last_tokens_size, float penalty);
    public static native void llama_sample_repetition_penalty(llama_context ctx, llama_token_data_array candidates, @Cast("const llama_token*") int[] last_tokens, @Cast("size_t") long last_tokens_size, float penalty);

    /** \details Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details. */
    public static native void llama_sample_frequency_and_presence_penalties(llama_context ctx, llama_token_data_array candidates, @Cast("const llama_token*") IntPointer last_tokens, @Cast("size_t") long last_tokens_size, float alpha_frequency, float alpha_presence);
    public static native void llama_sample_frequency_and_presence_penalties(llama_context ctx, llama_token_data_array candidates, @Cast("const llama_token*") IntBuffer last_tokens, @Cast("size_t") long last_tokens_size, float alpha_frequency, float alpha_presence);
    public static native void llama_sample_frequency_and_presence_penalties(llama_context ctx, llama_token_data_array candidates, @Cast("const llama_token*") int[] last_tokens, @Cast("size_t") long last_tokens_size, float alpha_frequency, float alpha_presence);

    /** \details Apply classifier-free guidance to the logits as described in academic paper "Stay on topic with Classifier-Free Guidance" https://arxiv.org/abs/2306.17806
     *  @param candidates A vector of {@code llama_token_data} containing the candidate tokens, the logits must be directly extracted from the original generation context without being sorted.
     *  \params guidance_ctx A separate context from the same model. Other than a negative prompt at the beginning, it should have all generated and user input tokens copied from the main context.
     *  \params scale Guidance strength. 1.0f means no guidance. Higher values mean stronger guidance. */
    public static native void llama_sample_classifier_free_guidance(
                  llama_context ctx,
                llama_token_data_array candidates,
                  llama_context guidance_ctx,
                                 float scale);

    /** \details Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits. */
    public static native void llama_sample_softmax(llama_context ctx, llama_token_data_array candidates);

    /** \details Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751 */
    public static native void llama_sample_top_k(llama_context ctx, llama_token_data_array candidates, int k, @Cast("size_t") long min_keep);

    /** \details Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751 */
    public static native void llama_sample_top_p(llama_context ctx, llama_token_data_array candidates, float p, @Cast("size_t") long min_keep);

    /** \details Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/. */
    public static native void llama_sample_tail_free(llama_context ctx, llama_token_data_array candidates, float z, @Cast("size_t") long min_keep);

    /** \details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666. */
    public static native void llama_sample_typical(llama_context ctx, llama_token_data_array candidates, float p, @Cast("size_t") long min_keep);
    public static native void llama_sample_temperature(llama_context ctx, llama_token_data_array candidates, float temp);

    /** \details Apply constraints from grammar */
    public static native void llama_sample_grammar(llama_context ctx, llama_token_data_array candidates, @Const llama_grammar grammar);

    /** \details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
     *  @param candidates A vector of {@code llama_token_data} containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
     *  @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
     *  @param eta The learning rate used to update {@code mu} based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause {@code mu} to be updated more quickly, while a smaller learning rate will result in slower updates.
     *  @param m The number of tokens considered in the estimation of {@code s_hat}. This is an arbitrary value that is used to calculate {@code s_hat}, which in turn helps to calculate the value of {@code k}. In the paper, they use {@code m = 100}, but you can experiment with different values to see how it affects the performance of the algorithm.
     *  @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy ({@code 2 * tau}) and is updated in the algorithm based on the error between the target and observed surprisal. */
    public static native @Cast("llama_token") int llama_sample_token_mirostat(llama_context ctx, llama_token_data_array candidates, float tau, float eta, int m, FloatPointer mu);
    public static native @Cast("llama_token") int llama_sample_token_mirostat(llama_context ctx, llama_token_data_array candidates, float tau, float eta, int m, FloatBuffer mu);
    public static native @Cast("llama_token") int llama_sample_token_mirostat(llama_context ctx, llama_token_data_array candidates, float tau, float eta, int m, float[] mu);

    /** \details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
     *  @param candidates A vector of {@code llama_token_data} containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
     *  @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
     *  @param eta The learning rate used to update {@code mu} based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause {@code mu} to be updated more quickly, while a smaller learning rate will result in slower updates.
     *  @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy ({@code 2 * tau}) and is updated in the algorithm based on the error between the target and observed surprisal. */
    public static native @Cast("llama_token") int llama_sample_token_mirostat_v2(llama_context ctx, llama_token_data_array candidates, float tau, float eta, FloatPointer mu);
    public static native @Cast("llama_token") int llama_sample_token_mirostat_v2(llama_context ctx, llama_token_data_array candidates, float tau, float eta, FloatBuffer mu);
    public static native @Cast("llama_token") int llama_sample_token_mirostat_v2(llama_context ctx, llama_token_data_array candidates, float tau, float eta, float[] mu);

    /** \details Selects the token with the highest probability. */
    public static native @Cast("llama_token") int llama_sample_token_greedy(llama_context ctx, llama_token_data_array candidates);

    /** \details Randomly selects a token from the candidates based on their probabilities. */
    public static native @Cast("llama_token") int llama_sample_token(llama_context ctx, llama_token_data_array candidates);

    /** \details Accepts the sampled token into the grammar */
    public static native void llama_grammar_accept_token(llama_context ctx, llama_grammar grammar, @Cast("llama_token") int token);

    // Performance information
    public static native @ByVal llama_timings llama_get_timings(llama_context ctx);
    public static native void llama_print_timings(llama_context ctx);
    public static native void llama_reset_timings(llama_context ctx);

    // Print system information
    public static native @Cast("const char*") BytePointer llama_print_system_info();

    // Set callback for all future logging events.
    // If this is not called, or NULL is supplied, everything is output on stderr.
    public static native void llama_log_set(llama_log_callback log_callback, Pointer user_data);

// #ifdef __cplusplus
// #endif

// Internal API to be implemented by llama.cpp and used by tests/benchmarks only
// #ifdef LLAMA_API_INTERNAL

// #endif // LLAMA_API_INTERNAL

// #endif // LLAMA_H


}
